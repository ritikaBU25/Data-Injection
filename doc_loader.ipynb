{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c34f5494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1fde5533240>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('Demo.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00280628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Demo.txt'}, page_content='Attended Agentic AI Session by Himanshu\\nPrepared Orion use case ppt\\nPresented Orion use cases using PPT\\nRevised logging module and log levels\\nRevised Pydantic BaseModel and field validation\\nSolved Leetcode')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa22a077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1fdf563ee90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Compliance.pdf')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3e8484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='1) List relevant regulatory requirements and how they are addressed \\nOrion’s compliance baseline is organized around four core regulatory intent areas that \\ncommonly apply to agentic AI solutions: (a) personal data protection and privacy, (b) \\nhealth-related data protection (where applicable), (c) electronic records and signatures \\n/ auditability, and (d) AI governance and risk management. For each intent area Orion \\nimplements control families that satisfy the objectives of regulation and good-practice \\nguidance: policy & governance, technical controls, operational controls, and evidence \\n& reporting. \\n• Policy & Governance — Define ownership and responsibilities (Compliance Owner, \\nData Owner, System Owner), maintain a documented Solution Profile (single-source-of-\\ntruth for the system), and operate a formal risk register and change control process so \\nevery design change is risk-assessed and approved. \\n• Technical Controls — Enforce strong encryption at rest and in transit, role-based \\naccess control (least privilege), key management, pseudonymization or tokenization of \\nidentifiers, and data residency configuration to limit where data may be stored or \\nprocessed. \\n• Operational Controls — Implement human-in-the-loop review gates for sensitive \\noutputs, routine compliance training for staff, periodic control self-assessments, \\nscheduled penetration and security testing, and a documented incident response / \\nescalation process. \\n• Evidence & Reporting — Retain immutable audit logs, produce periodic compliance \\nreports, and maintain versioned artifacts (design docs, test reports, validation \\nevidence) to demonstrate adherence during internal or regulatory reviews. \\n(These control families map to legal/regulatory objectives such as privacy, security, \\nrecord integrity and AI risk management; see full regulation texts and guidance for exact \\nrequirements.) GDPR+2HHS.gov+2 \\n \\n2) Comprehensive Error Handling — design and operational requirements \\nRobust error handling is essential for safe, auditable operation of an agentic AI system. \\nOrion’s approach treats error handling as a first-class compliance control that supports \\navailability, traceability and safe failure. \\nDesign principles \\n• Fail-safe defaults: If a component fails or behavior is uncertain, the system \\nshould prefer safe, minimal actions (e.g., halt an automated JIRA commit and \\nsurface a human review item instead).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='• Graceful degradation: Non-critical features degrade first; core safety and \\ncompliance features (sanitization, consent checks, audit logging) remain active \\neven under partial failure. \\n• Clear classification and routing: Errors are classified (transient network, rate-\\nlimited API, permanent configuration failure, data validation error) and routed to \\nappropriate handlers (automatic retry, queued reprocessing, human escalation). \\nImplementation controls \\n• Centralized error management service that normalizes error events from \\nagents, produces structured error records (error code, component, time, input \\ncontext, stack), and forwards them to the incident queue. \\n• Automated triage rules that map classes of errors to actions (retry, queue, \\nrollback, alert). Retries use backoff policies (see next section). Permanent \\nfailures produce structured remediation tasks (with owner and SLA). \\n• Observability & runbooks — every error type has an associated runbook (steps \\nto investigate, revert, restore, and report). Key metrics (error rate, time to \\nremediation, reprocessing success rate) are instrumented and monitored. \\n• Data protection on failure — when errors involve data (e.g., partial writes), \\nOrion applies transactional or compensating operations to avoid leaving PII/PHI \\nin an inconsistent or exposed state. \\nOperational controls \\n• Automated alerts for thresholds (error spikes, queue growth) routed to on-call \\nteams with escalation rules. \\n• Weekly error reviews to identify systemic issues and update runbooks. \\n• Preservation of input context (masked where necessary) in the error record so \\nteams can troubleshoot without exposing sensitive data. \\n \\n3) Retry Mechanisms — exponential backoff + jitter best practice and configuration \\nrecommendations \\nFor transient failures (temporary network blips, rate limit responses, brief downstream \\noutages) Orion uses exponential backoff with jitter to maximize reliability while \\nminimizing load amplification. \\nWhy exponential backoff + jitter \\n• Exponential backoff spaces retries so the system does not continuously hammer \\na recovering endpoint; jitter (randomized delay) prevents many clients from'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='synchronizing retries and causing a “retry storm. ” This pattern is recommended \\nby major cloud providers and architecture guides. Amazon Web Services, Inc. \\nRecommended configuration pattern (example defaults) \\n• Max attempts: 5 (configurable per integration) \\n• Base delay: 200 ms \\n• Backoff factor: 2 (delay doubles each retry) \\n• Jitter: use full jitter or decorrelated jitter (i.e., randomize delay between 0 and \\ncurrent backoff interval) to avoid clumping \\n• Max backoff cap: 30 seconds (prevent indefinite waits) \\n• Retryable error classes: transient HTTP 408, 429, 5xx; network timeouts; \\nconnection reset; transient database or message broker errors. Do not retry on \\n4xx errors that indicate client fault (unless explicitly idempotent and safe). \\n• Idempotency & safe retries: for any retried operation, design APIs and \\noperations to be idempotent or employ a request-idempotency key to prevent \\nrepeated side effects (e.g., duplicate JIRA issues). \\nImplementation notes \\n• Use built-in client SDK retry functionality when available (they often implement \\nrecommended backoff/jitter) and instrument the retry events in metrics (retry \\ncount histogram, retry latency distribution). \\n• Log each retry attempt with correlation id and original input context (masked as \\nrequired) so retries are auditable and diagnosable. \\n• Provide circuit-breaker logic: if a downstream service fails repeatedly, trip the \\ncircuit (stop retries temporarily) and open an alert so operators can investigate. \\n \\n4) Audit Trail Logs — design for completeness, immutability, and forensic readiness \\nAuditability is a cornerstone of compliance. Orion designs its logging and audit trail \\nsystem so it is comprehensive, tamper-evident, and queryable for investigations and \\nregulatory review. \\nWhat to log (minimum set) \\n• Identity of actor (human user id or agent id), role and authentication context. \\n• Action performed (create/update/delete/approve/execute), including object \\nidentifiers (e.g., solution profile id, JIRA ticket id).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='• Full event timestamp (ISO8601 with timezone) and source system/component. \\n• Pre- and post-state for critical changes (before/after snapshot or a secure diff). \\n• Correlation IDs to relate multi-step flows and upstream/downstream calls. \\n• Retention and disposition metadata (who archived the record, why, retention \\nexpiry). \\nImmutability & integrity \\n• Store audit logs in append-only, tamper-resistant storage (WORM / write-once \\nstorage, or digital signatures/hashes anchored to a secure ledger). Adopt \\nperiodic archival to immutable backups that are integrity-checked. NIST \\nguidance highlights write-once media and digital signatures as ways to prevent \\nlog tampering. NIST Publications \\nAccess & encryption \\n• Encrypt logs at rest and in transit. Restrict read access to authorized compliance \\nand audit roles; restrict write access to trusted system services only. Maintain \\nstrong logging-service authentication and rotate log-access credentials \\nfrequently. \\nMonitoring & alerting \\n• Deploy automated integrity checks (periodic hash comparisons, verified \\nbackups) and alerts for anomalous deletion attempts, sudden log volume \\nspikes, or gaps in expected event sequences. \\n• Provide an audit-query interface for authorized reviewers that supports \\nreconstructed timelines and exportable evidence packages (signed and time-\\nstamped). \\nRetention & legal hold \\n• Implement configurable retention policies per data classification. When an \\ninvestigation or legal preservation is required, add a legal hold to prevent \\ndeletion/archival of relevant logs. \\n \\n5) Quality Assurance — validation checkpoints throughout the workflow (practical \\npipeline) \\nQuality is enforced through a layered approach combining automated testing, \\nenvironment gating, human review and production monitoring. \\nPipeline & checkpoints'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='1. Pre-commit checks: linters and static analysis detect code style and certain \\nclasses of security issues before code merges. \\n2. CI automated tests: every commit triggers unit tests and integration tests to \\nvalidate functional correctness and catch regressions. Include schema, \\ncontract, and input-sanitization tests for AI prompt processing and output \\nformatting. \\n3. Stage / pre-production: deploy release candidate to a staging environment that \\nmirrors production (data subset or synthetic data). Run end-to-end workflows, \\nperformance tests, and security scans here. Use synthetic or pseudonymized \\ndata to avoid exposing real PII/PHI. \\n4. Human acceptance / compliance review: for changes that affect compliance \\ncontrols (data handling, consent flows, AI sanitization, auditing), require sign-off \\nby compliance or quality owners before deployment. This human-in-the-loop \\nstep validates non-functional requirements (auditability, traceability, masking). \\n5. Canary / progressive rollout: release to a small segment of traffic and monitor \\nkey metrics (error rate, latency, data leak indicators, unusual content generation) \\nbefore full rollout. \\n6. Post-deployment verification: run smoke tests and automated consistency \\nchecks; confirm audit logs were generated for the run; verify that key control \\npoints (consent checks, sanitization) functioned as expected. \\n7. Production monitoring and continuous feedback: instrument telemetry \\n(SLOs/SLA metrics, error budgets, data quality, drift indicators) and feed back \\nresults into the development backlog and model/prompt governance process. \\nTest types and coverage \\n• Unit tests for deterministic logic and sanitization functions. \\n• Integration tests for agent orchestration flows (e.g., prompt → agent decisions → \\nJIRA API interactions). \\n• Contract tests for API compatibility with external services. \\n• Security tests including SAST, DAST and periodic penetration tests. \\n• Data quality tests to ensure data pipeline transformations do not leak identifiers \\nor corrupt content. \\nGovernance & evidence'),\n",
       " Document(metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2025-10-14T15:49:07+05:30', 'author': 'Ritika Kulkarni', 'moddate': '2025-10-14T15:49:07+05:30', 'source': 'Compliance.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='• Maintain a test evidence repository (test plans, results, sign-offs) linked to each \\nrelease and solution profile. This evidence is part of the compliance package for \\naudits. \\n• Enforce minimum test coverage thresholds and failure-blocking policies for \\nreleases that touch sensitive controls. \\n \\nPractical operational items to include in Orion’s SOPs / Runbooks \\n• Metric dashboards: Retry rates, error rates per component, mean time to \\nremediation, audit log integrity check results, percent of outputs flagged for \\nhuman review. \\n• Runbooks & playbooks: Clear step-by-step remediation instructions for \\ncommon failures, with assigned roles and escalation paths. \\n• Periodic reviews: Monthly error trend reviews, quarterly control self-\\nassessments, annual third-party audits. \\n• Data handling procedures: Masking/pseudonymization libraries, key \\nmanagement schedule, data residency enforcement checklist. \\n \\nKey sources (authoritative guidance) \\n• GDPR reference text (for data-protection objectives and rights). GDPR \\n• HIPAA/HHS guidance (technical safeguards overview and encryption \\nrecommendations). HHS.gov \\n• FDA guidance on electronic records and signatures (Part 11) for audit and record \\nintegrity expectations. U.S. Food and Drug Administration \\n• NIST guidance on log management and audit trail integrity (write-once, \\nsignatures). NIST Publications \\n• Exponential backoff + jitter best practices (AWS builders guidance). Amazon \\nWeb Services, Inc.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF_documents=loader.load()\n",
    "PDF_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584e4365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(PDF_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee452c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1fdf5e96060>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "webdoc=WebBaseLoader(web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/',), \n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "                         \n",
    "                     )))\n",
    "                     \n",
    "webdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3460d8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webdoc.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0f9a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2025-01-06', 'Title': 'Foundations of GenIR', 'Authors': 'Qingyao Ai, Jingtao Zhan, Yiqun Liu', 'Summary': 'The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.'}, page_content='Foundations of GenIR\\nQingyao Ai1†, Jingtao Zhan1†, Yiqun Liu1\\n1Dept. of Computer Science and Technology, Tsinghua University,\\nBeijing, China.\\nContributing authors: aiqy@tsinghua.edu.cn;\\nzhanjt20@mails.tsinghua.edu.cn; yiqunliu@tsinghua.edu.cn;\\n†These authors contributed equally to this work.\\nAbstract\\nThe chapter discusses the foundational impact of modern generative AI models\\non information access (IA) systems. In contrast to traditional AI, the large-scale\\ntraining and superior data modeling of generative AI models enable them to pro-\\nduce high-quality, human-like responses, which brings brand new opportunities\\nfor the development of IA paradigms. In this chapter, we identify and introduce\\ntwo of them in details, i.e., information generation and information synthesis.\\nInformation generation allows AI to create tailored content addressing user needs\\ndirectly, enhancing user experience with immediate, relevant outputs. Information\\nsynthesis leverages the ability of generative AI to integrate and reorganize exist-\\ning information, providing grounded responses and mitigating issues like model\\nhallucination, which is particularly valuable in scenarios requiring precision and\\nexternal knowledge. This chapter delves into the foundational aspects of gener-\\native models, including architecture, scaling, and training, and discusses their\\napplications in multi-modal scenarios. Additionally, it examines the retrieval-\\naugmented generation paradigm and other methods for corpus modeling and\\nunderstanding, demonstrating how generative AI can enhance information access\\nsystems. It also summarizes potential challenges and fruitful directions for future\\nstudies.\\nThe primary distinction between modern generative models and traditional AI tech-\\nniques lies in their capability to generate complicated and high-quality output based\\non human instructions. As shown by many studies [1–3], modern generative AI models\\npossess remarkable abilities to generate responses that closely mimic human inter-\\naction. General speaking, such impressive performance comes from their large-scale\\n1\\narXiv:2501.02842v1  [cs.IR]  6 Jan 2025\\ntraining collections and their advanced data modeling algorithms. Their superior data\\nunderstanding ability can benefit almost every components of existing information\\naccess systems, from document encoding and index construction, to query process-\\ning and relevance analysis, etc. However, when talking about new opportunities or\\nparadigms that are uniquely brought by the generative AI to information access, they\\ncan be broadly categorized in two directions. The first one is to create content that\\ndirectly addresses user’s information needs. By understanding and taking user queries\\nas input instructions, generative AI models are able to generate specific answers or\\nproducts tailored to the individual’s request. This direct approach to information gen-\\neration can significantly enhance user experience by providing immediate and relevant\\nresponses. The second direction is to leverage the advanced instruction-following capa-\\nbilities of generative AI models to synthesize and recombine existing information in\\ninnovative ways. Generative AI such as large language models (LLMs) can take exist-\\ning data and transform it into new, coherent pieces of information that may not have\\nbeen explicitly outlined before. This ability to reinterpret and organize information\\nopens up new possibilities for retrieval system design and applications. Therefore, in\\nthis chapter, we discuss how generative AI models could help information access from\\ntwo perspectives, namely information generation and information synthesis.\\n1 Information Generation\\nInformation need is diverse and typically long-tail. Traditional information retrieval\\nsystems, such as search engines and recommendation platforms, are designed to present\\ninformation that already exists. However, these systems often fall short when it comes\\nto fulfilling the less common information needs. This is particularly evident in scenarios\\nrequiring creative creation, where users seek not just information but inspiration and\\nnovel ideas. The limitations of traditional information systems in addressing these\\nunique demands have paved the way for the emergence of generative models, which\\nhold the promise of creating new information that aligns closely with the long-tail\\ninformation needs.\\nIn recent years, generative models have made significant developments. For\\ninstance, ChatGPT can respond to user questions, Bing enhances its responses\\nwith retrieval-augmented generation, and Midjourney generate images based on user\\nprompts, and recommendation systems generate personal contents for different users.\\nThe development is mainly driven by the capable model architectures, computational\\nresources, and the large-scale internet data. These elements have facilitated the per-\\nformance of generative models to new heights. With the continuous efforts on scaling\\nup these elements, the model performance is still rapidly improving. Nowadays, gener-\\native models have gradually been integrated into various workflows and everyday life\\nactivities.\\nIn this section, we present the foundation of generative models. This section is\\norganized as follows. Section 1.1 shows the efforts on designing the model architectures\\nfor large language models. Section 1.2 discusses how scaling facilitates the development\\nof generative models and its potential future. Section 1.3 presents the different training\\nstages of large language models. Finally, Section 1.4 introduces how large language\\nmodels are used in multi-modal scenarios.\\n2\\n1.1 Model Architecture\\nIn different generation scenarios like ChatGPT or SoRA, Transformer [4] has emerged\\nas the predominant model structure. It starts with an embedding layer, followed by\\nmultiple neural layers. Within each layer, an attention mechanism models the interac-\\ntions between words, creating contextualized embeddings. The final decision on word\\ngeneration probabilities is derived by comparing the output embedding with the vocab-\\nulary embeddings. We illustrate the model architecture in Figure 1. Unlike traditional\\nRecurrent Neural Networks [5], Transformers are capable of modeling long-distance\\ninteractions between words directly, which provides a more powerful representational\\ncapability. Numerous enhancements to the Transformer architecture have been pro-\\nposed. In the following, we will explore various modifications to each component of the\\nTransformer, highlighting the advancements that have further improved its efficacy\\nand efficiency.\\nTransformer\\n1\\n2\\n3\\n4\\nOutput\\nTokens\\nPositions\\nHidden \\nStates\\nAttention\\nFeed \\nForward\\nTransformer Layer\\nFig. 1 Transformer architecture: the overview on the left and the illustration of one layer on the\\nright [4].\\n1.1.1 Word Embedding\\nWord embedding module is at the bottom of the Transformer architecture. Initially, a\\ntokenizer breaks down a sentence into tokens, which the Word embedding module then\\nmaps into embeddings. These are combined with position embeddings and fed into\\nsubsequent neural layers. Recent research on large-scale language models has identified\\nword embeddings as one of the main sources to training instability [6]. Particularly\\nin the early stages of training, the gradients of word embeddings are often orders of\\nmagnitude larger than those of other parameters. To address this issue, Le Scao et al.\\n[7] introduced a layer normalization immediately after the word embedding layer,\\nstabilizing the distribution effectively. Besides, Zeng et al. [6] opted to scale down the\\ngradients of the word embeddings by an order of magnitude to prevent substantial\\nupdates. Both approaches have been proven effective in stabilizing the training of\\nlanguage models at the 100 billion parameter scale. Yet, whether they are still effective\\nfor larger models remains to be investigated.\\n3\\n1.1.2 Position Embedding\\nPosition embedding is essential for Transformer. Unlike RNNs that inherently process\\nsequences in order, vanilla attention mechanism disregards the positional distances\\nbetween words and Transformer has to rely on position embeddings for position mod-\\neling. Initially, Transformer [4] utilized Sinusoidal embeddings, a non-trainable form\\nof position embedding that is added directly to word embeddings. Later, Devlin et al.\\n[8] introduced trainable position embeddings, which is initialized randomly and are\\nupdated through gradient descent during training. Subsequently, Raffel et al. [9] and\\nPress et al. [10] proposed relative positioning, where the attention mechanism incorpo-\\nrates biases based on the relative positions of words to better model varying distances.\\nRecently, Su et al. [11] introduced the concept of rope position embedding, based on\\nthe principle that the dot product of vectors correlates with their magnitudes and the\\nangles between them. By rotating vectors in space proportionally to their positions,\\nthis method naturally integrates positional information into attention scores. Black\\net al. [12] has found that this approach outperforms trainable position embeddings.\\nYet, these approaches may not work well when extrapolated to long sequences and\\nmore effective methods need to be explored.\\n1.1.3 Attention\\nThe attention mechanism models interactions between words and is a significant com-\\nponent of the Transformer architecture. Enhancements to the attention module have\\npredominantly focused on two aspects: modeling long texts and optimizing the Key-\\nValue (KV) cache. (1) Modeling Long Texts: The vanilla attention mechanism has\\na complexity of O(n2), which significantly increases computational costs for long\\ntexts. To address this, Sparse Transformer [13] employs sparse attention, utilizing pre-\\ndesigned attention patterns to avoid the computation of attention over long sequences.\\nAnother approach, Reformer [14], uses Locality-Sensitive Hashing (LSH) to reduce\\ncomputational complexity. Additionally, Munkhdalai et al. [15] compressed context\\ninformation to shorten sequences, thereby reducing overhead. Others have explored\\nretrieval-based methods [16, 17]. This area of research continues to hold considerable\\npotential for future advancements. (2) Optimizing KV Cache: classic Transformers\\nuse multi-head attention (MHA), which requires storing extensive key-value caches\\nduring inference, slowing down model generation. To mitigate this, Shazeer [18] pro-\\nposed multi-query attention, which employs multiple key heads but only a single\\nvalue head, substantially reducing the key-value cache and enhancing computational\\nspeed. However, Ainslie et al. [19] found that this could degrade model performance,\\nleading to the development of grouped query attention. This method allows multiple\\nkey heads to share a single value head, effectively serving as a hybrid between MQA\\nand MHA, balancing computational complexity and performance more effectively.\\nRecently, DeepSeek-AI [20] introduced multi-head latent attention, which compresses\\nkeys and values into a single latent space, thereby reducing the key-value cache while\\nmaintaining robust representational capacity.\\n4\\n1.1.4 Layer normalization\\nLayer normalization (LayerNorm) is important for stabilizing the distribution of\\nhidden states, a key to train large language models. In the classical Transformer archi-\\ntecture, LayerNorm is positioned between residual blocks, hence termed Post-LN.\\nResearchers [21] observed that this configuration could lead to high gradients near\\nthe output layers and very small gradients near the input layers, resulting in unstable\\ngradients and challenging training dynamics. To address this issue, the Pre-LN configu-\\nration was proposed [21], placing LayerNorm on the residual pathways before attention\\nor feed-forward network (FFN) module. Experiments have shown that this adjustment\\nleads to more uniform gradient distribution. Building upon Pre-LN, other researchers\\nintroduced Sandwich-LN [22], which adds an additional LayerNorm at the output of\\nthe residual pathways, further enhancing the training stability. Beyond merely adjust-\\ning the position of LayerNorm, researchers have developed DeepNorm [23], which\\ncombines a tailored parameter initialization strategy with modified residual connec-\\ntions to stabilize training. This approach enables the training of Transformers with\\ndepths reaching up to 1000 layers. Nevertheless, there still lacks a theoretical under-\\nstanding about how layer normalization affects the training stability and more work\\nneeds to be done for scaling the model even further.\\n1.2 Scaling\\nAcross different information generation scenarios, scaling has been a siginificant factor\\nto the performance improvement. It is largely attributed to the discovery of scaling\\nlaws [24]. Scaling laws describe how loss decreases in a log-linear manner as model size\\nor training data volume increases. It can be formulated as follows:\\nL(x) = L∞+ k · x−α,\\n(1)\\nwhere L is the loss, x is model size or data size, and k and α are coefficients. This scaling\\nformula has become a crucial theoretical guide in the era of large models, suggesting\\nthat performance can be enhanced at a log-linear rate simply by scaling up the model\\nsize or training data. Based on these scaling laws, researchers also derived optimal\\nmodel sizes given fixed computational resources [25]. Their findings indicate that as\\ncomputational capacity expands, it is beneficial not only to increase the training step\\nbut also the model size. This insight has further facilitated the pursuit of large models.\\nThe correctness of scaling laws was first proposed in language modeling field and then\\nvalidated in many other areas, including data mixture scaling laws [26], multimodal\\nscaling laws [27], and scaling laws specific to information retrieval [28].\\nDespite wide recognition of scaling laws, there remains disagreement among\\nresearchers about whether scaling is the correct path to the future. This stems from\\ntwo main concerns: the uncertain relationship between loss and practical metrics, and\\nthe inference costs associated with large models.\\n• Loss vs. Metric Improvement: The first arguing point is whether a linear reduction\\nin loss can translate into super-linear improvements in actual metrics. If metrics\\ncould improve super-linearly with linear increases in computational effort, scaling\\n5\\nup models would be highly advantageous. However, if the decrease in loss only\\nresults in linear or sublinear metric improvements, the diminishing improvements\\nmake scaling an inefficient option. The relationship between loss and metric per-\\nformance remains an open question. Some researchers [29] believe that metrics can\\nimprove super-linearly, which is termed emergent abilities. This is further supported\\nby Du et al. [30], who observed a jump in metrics when loss reaches a certain thresh-\\nold. Additionally, Power et al. [31] introduced the concept of “grokking” to explain\\nemergence, showing that models might suddenly exhibit strong generalization capa-\\nbilities when provided with sufficient computational resources. Nevertheless, some\\nresearchers [25] argued that such phenomena do not exist, showing that a well-\\ntrained smaller model can outperform a larger, undertrained one. Schaeffer et al.\\n[32] demonstrated that emergent abilities are artifacts of discrete metric functions\\nand found that continuous metric functions do not exhibit such behaviors. McKen-\\nzie et al. [33] even found that scaling results in worse metric scores. The existence of\\nspecific emergent abilities remains unresolved and needs to be investigated in future\\nwork.\\n• Inference Cost Considerations: Early studies on scaling laws did not account for the\\nhigher inference costs associated with larger models. Thus, the arguments that larger\\nmodels are better [25] do not apply when the inference costs are considered. Instead,\\nsmall models demonstrate potential to lower the inference costs. As shown by Fang\\net al. [28], the optimal model sizes become significantly smaller when accounting\\nfor inference costs. Besides, Mei et al. [34] show that smaller models can utilize\\nmore sampling steps during inference and thus perform better. Consequently, many\\nrecent studies focus on extensively training small models. For example, Llama [3]\\nand MiniCPM [35] are trained with data and steps that far exceed the guidance\\nsuggested by scaling laws. In the future, the models may be used on a phone to\\nbuild up intelligent interaction with users. Thus, it is important to develop high-\\nperforming small models.\\n1.3 Training\\nGenerative models in different scenarios are similar in training. For example, they\\nusually use autoregressive training objectives, pretraining-sft-rlhf training stages, and\\nprompt tuning procedure. In this section, we focus on the text generation scenario. We\\nfirst discuss the training objectives and then show the three training stages. Finally,\\nwe discuss how to design the prompts after the model is trained.\\n1.3.1 Training Objectives\\nFor generative language models, the training objective is usually next token prediction.\\nHowever, this was not widely used when Transformers first appeared. Initially, masked\\nlanguage modeling was the prevalent training objective during the BERT era [8]. It\\nmasks 15% of the words in a text randomly, and the model is tasked with predicting\\nthese masked words. This approach allows the model to utilize bidirectional attention,\\nenhancing its representational capabilities. Even today, BERT models perform bet-\\nter than autoregressive models on tasks requiring bidirectional attention. However, a\\n6\\nsignificant drawback of this method is the gap between its training setup and down-\\nstream tasks, necessitating a fine-tuning phase for adaptation to various applications.\\nThus, its zero-shot generalization capabilities are very limited.\\nNext token prediction was developed to address the inability of masked language\\nmodeling to generalize zero-shot to downstream tasks. The authors of GPT-2 [36]\\nproposed that all natural language processing tasks could be reformulated as next\\ntoken prediction tasks. By training models on this task, models could be directly\\napplied to any downstream task without the need for specific fine-tuning. In fact,\\nresearch nowadays demonstrates the effectiveness of this idea. Mathematically, next\\ntoken prediction can be represented with the following formula:\\nP (xt+1 | x1, . . . , xt) ,\\n(2)\\nwhich is to predict the probability of the next token xt+1 given the sequence of previous\\ntokens.\\n1.3.2 Training Stages\\nThe training process of language models typically unfolds in three stages: pre-training,\\nsupervised fine-tuning (SFT), and reinforcement learning from human feedback\\n(RLHF). Each phase presents unique challenges and methodologies.\\nPre-training is the most resource-intensive stage. It is training a randomly ini-\\ntialized model on a large dataset to develop a robust linguistic capability. Several\\nchallenges arise during this stage: (1) Large models are especially difficult to train\\nfrom random initialization. During training, there are often spikes in training loss or\\ndifficulty in converging [6, 23, 37]. We discussed various architectural improvements\\nin Section 1.1 to address these instabilities, yet a definitive solution remains an open\\nissue. (2) The computational demand is substantial. Pre-training requires stable and\\nefficient use of computational resources [1]. It often involves parallel processing across\\nmultiple machines, which can lead to low utilization rates of computing resources [38].\\nZeng et al. [6] reported numerous hardware failures during pre-training. (3) The qual-\\nity of pre-training data is crucial [39]. Given the vast amount of data needed, efficiently\\nfiltering out low-quality data is essential. The filtering methods usually employ neural\\nscoring models and based on the credibility of the site [40, 41].\\nSupervised Fine-Tuning (SFT) is to train the model on instruction-response\\npairs [42]. The model can thus learns to follow instructions or engage in dialogue [3]. To\\nenhance dataset diversity, researchers often leverage different types of NLP tasks. The\\nquality of the dataset is significant and requires a skilled annotation team. Besides, it\\nis also important to label safety-related data, which helps instruct the models to learn\\nto reject inappropriate requests [3].\\nReinforcement Learning from Human Feedback (RLHF) focuses on aligning the\\nmodel with human preferences based on human feedback [43, 44]. The process starts\\nby sampling real human prompts to which the model generates multiple responses.\\nThese responses are then compared by users or third-party annotators. A reward model\\nis trained based on these human preferences. Subsequently, reinforcement learning\\n7\\ntechniques utilize the reward model to guide the model updates. This approach sig-\\nnificantly enhances the quality of model outputs, especially in creative writing tasks.\\nHowever, a major challenge is the generalizability of the reward model; as the model\\nevolves, the reward model may no longer accurately assess the quality of outputs.\\nContinuous iterations of this process are necessary to mitigate this issue [3]. Recently,\\nthere are also some offline reinforcement learning algorithms that do not necessaite\\ntraining a reward model, such as DPO [45]. Yet studies [46] show that such offline\\nlearning methods still underperform the online learning methods.\\n1.3.3 Prompt Optimization\\nGenerative models are highly sensitive to the input prompts; an effective prompt can\\nsignificantly enhance the quality of the model’s output [47]. Therefore, optimizing\\nprompts for a generative model is a crucial area of research. Here are three main\\ndirections:\\n• Designing Prompt Templates: Researchers often design prompts that mimic\\nhuman thought processes to guide the model effectively. This includes using\\nstructured thought patterns like chain-of-thought [48], tree-of-thought [49], and self-\\nconsistency [50], which help the model organize and process information in a logical\\nmanner.\\n• Iterative Optimization of Prompt Templates: like reinforcement learning, this\\nmethod continuously iterate and refine the prompt templates based on the gen-\\neration feedback. Given that prompt templates are typically discrete, researchers\\nusually employ large language models to conduct prompt updates [51, 52].\\n• Training Prompt Rewriting Models Using User Interaction Logs: This approach\\nharnesses the rich feedback contained within user interaction logs to tap into user\\ninsights. By analyzing how users interact with the model, researchers can train an\\nautomated model to rewrite prompts more effectively. This method leverages real-\\nworld data to better align the prompts with user intentions and improve the model’s\\nresponses [53, 54].\\n1.4 Multi-modal Applications\\nThe rapid advancement of language models has significantly helped progress in the\\nmultimodal domain. Language models facilitate the understanding of multimodal\\ndata and developments in multimodal generation. We will discuss these two aspects\\nseparately.\\n1.4.1 Multi-modal Understanding\\nMultimodal Understanding involves models processing inputs from multiple modali-\\nties to produce relevant textual responses. For example, GPT-4o can process textual,\\nvisual, and auditory input. The challenges in this area include designing model struc-\\ntures that can handle multimodal inputs and crafting appropriate training objectives.\\nHere, we focus on how visual signals are integrated into large language models:\\nIn terms of aligning multimodal inputs, there are mainly three approaches:\\n8\\n• Object Detection-Based Input: This method involves detecting objects within an\\nimage, extracting their features and associated spatial information, and then feeding\\nthis data into the language model [55, 56]. While this approach is effective, it tends\\nto be slow due to the processing time required for object detection.\\n• Visual Encoding: Another method encodes images directly using a visual encoder,\\nwhich converts images into a latent vector representation before integration with\\nthe model [57–61]. This method can sometimes result in the loss of detail.\\n• Patch-Based Input: The most efficient approach involves dividing images into several\\npatches, transforming them with a simple linear layer, and directly inputting them\\ninto the model without the need for a complex visual encoder [62].\\nIn terms of training methods, there are mainly four types of training objectives:\\n• Contrastive Learning or Image-Text matching: These tasks require the model to\\ncorrectly categorize images and their corresponding textual descriptions, aligning\\nthe representations of text and images [61, 63, 64].\\n• Image Captioning: The model generates captions based on images, which helps it\\nlearn to understand the visual content [58–61].\\n• Fine-Grained Image Understanding: The model is tasked to describe specific areas\\nof an image or locate particular objects within an image. This helps enhance the\\nmodel’s detailed comprehension of visual elements [58, 65].\\n• Image Generation: This task is reconstructing the original pixels of an image that\\nhas been blurred or corrupted [58, 66].\\nThese methodologies and training objectives are crucial for advancing models’\\ncapabilities to process and interpret complex multimodal information effectively. This\\nfacilitate a more natural interaction with users.\\n1.4.2 Multi-modal Generation\\nMulti-modal generation models, such as text-to-image generation, have substantially\\nrevolutionized the field of art creation. Traditionally, GAN [67] and autoregressive\\nmethods [68] are mainstream methods. However, they are computationally expensive\\nand can not produce high-quality results. Recently, diffusion [69, 70] emerges as a new\\nstate-of-the-art method in multimodal generation. It perturbs the data with noise and\\nlearns to reconstruct the original data.\\nLanguage models are increasingly applied in the multimodal generation domain,\\nsuch as in image [71, 72] and video generation [73, 74]. Language models are primarily\\nutilized for processing training data and reformulating prompts.\\nIn terms of training data, the titles associated with real-world images or videos\\noften contain significant noise. If generative models are trained directly on these noisy\\ntitles, it could lead to inaccurate semantic understanding. To address this, language\\nmodels can be used to filter and regenerate text descriptions within the training\\ndata [75, 76]. For instance, a multimodal understanding model could first be trained,\\nthen used to relabel videos or images to obtain more precise and detailed text descrip-\\ntions. Experimental results have shown that this method significantly improves the\\nfidelity of model generations to prompts.\\n9\\nDuring inference, multimodal generation models are highly sensitive to the input\\nprompts. Many users do not know how to craft effective prompts and thus get unsat-\\nisfying responses [77]. As a result, it is common to train a language model to rewrite\\nuser-provided prompts to enhance the quality of the generated images [75]. One of\\nthe challenges here is the difficulty in annotating such rewriting training data, as even\\nsystem developers may not always know the optimal prompts, let alone crowd-sourced\\nworkers [78]. To overcome this, some researchers collect a large number of user-shared\\neffective prompts as training data [79]. Others build prompt-rewriting models based\\non user log data, capturing preferences and feedback for training [53].\\n2 Information Synthesis\\nOther than generating information directly, another important research and appli-\\ncation direction is to use the power of generative AI models, particularly LLMs, to\\nintegrate existing information and generate grounded responses accordingly. For sim-\\nplicity, we refer to this paradigm as information synthesis. The key difference between\\ninformation generation and information synthesis is the source of information. Infor-\\nmation generation relies on the internal knowledge and information gathered through\\nthe training of generative AI models to create the model outputs, while informa-\\ntion synthesis requires external sources to provide information to the models, and the\\nmodels serve more as a integrator than a creator. There are multiple reasons why\\ninformation synthesis is considered more reliable than generation in several IA sce-\\nnarios. Here we discuss two of the most significant ones, i.e., model hallucination and\\nexternal knowledge.\\nHallucinating, which refers to the behavior of generative AI models that create\\nresponses and outputs that are not grounded by facts or existing supporting materials,\\nis rooted in the foundation of most existing generative AI systems. For instance,\\nLLMs create responses based on the next token prediction task, which formulates the\\ngeneration of language as a probabilistic process and generates the next token in the\\noutput based on a probabilistic distribution (over the vocabulary) predicted by neural\\nnetworks [1, 3]. The probabilistic model of LLMs allows them to capture knowledge in\\nlarge scale data efficiently and effectively, but it also introduces inevitable variance in\\ntheir generation process. In other words, it is well acknowledged that it’s theoretically\\nimpossible to prevent LLMs from generate data that are not seen in their training\\nprocess [80]. While the ability of hallucinating is the source of creativity for LLMs (and\\nfor human as well), it’s not always desirable in practice, particularly for tasks with\\nhigh requirements on result precision, reliability, and explanability. Therefore, asking\\nthe generative AI models to integrate human created or factually grounded materials\\ninstead of generating information on their own is often considered more effective and\\nrobust to hallucination-sensitive applications.\\nThe need of external knowledge is another key reason why we may prefer informa-\\ntion synthesis over information generation. Despite the fact that modern generative\\nAI models are trained with incredibly large amount of data gathered from the Web,\\nthere are many cases where we still need to retrieve and find supports from external\\n10\\nknowledge collections to finish certain tasks. Examples including the use of pri-\\nvate datasets, vertical domain applications that require special knowledge, tasks that\\ninvolve time-sensitive data, etc. It is usually inefficient or prohibitive to update large-\\nscale generative AI models such as LLMs with task-oriented external data through\\nmodel pre-training or supervised fine-tuning (SFT) [81–83]. Even if possible, such\\nparadigm is not preferred because the internal knowledge structures of most genera-\\ntive AI models are still mystery (at least of today), and there is no guarantee that\\nthe models could behavior and use the external information as we expect. In con-\\ntrast, using generative AI models as information synthesizer gives us not only more\\nflexibility, but also more transparency and control over system outputs.\\nIn this section, we discuss how generative AI models, particularly LLMs, can serve\\nas effective information synthesizers for IA. We start with introducing one of the most\\npopular information synthesis paradigm, i.e., retrieval augmented generation (RAG),\\nand then discuss several other directions that utilize LLMs for corpus modeling and\\nunderstanding.\\n2.1 Retrieval Augmented Generation\\nRetrieval Augmented Generation, or RAG, refers to the process of augmenting LLMs\\nwith data retrieved from external collections or synthesizing multiple retrieval results\\nwith LLMs for downstream applications [84, 85]. While the popularity of RAG rose\\nafter the release of large-scale pre-trained language models such as GPT [1] and\\nBART [86], relevant topics and techniques have already been studied for at least more\\nthan two decades in both IR and NLP communities, e.g., extractive and abstractive\\nsummarization that generates summary based on retrieved sentences [87, 88] or answer\\nextraction from top retrieved document [89]. A major reason why RAG-like techniques\\nwere not as attractive as they are today is the limited performance of generative mod-\\nels before the era of LLMs. After ChatGPT [1] demonstrated superior ability text\\ngeneration at the end of 2022, there have been many studies and surveys on RAG and\\nits applications in LLMs [84, 90, 91]. As the intent of this chapter is not to provide yet\\nanother survey on existing RAG papers, we focus the following discussions on several\\npresent and future directions for RAG and their relations underneath.\\n2.1.1 Naive RAG\\nNaive RAG refers to the paradigm that directly feeds documents or other types of\\ninformation retrieved by a retrieval system to the input (e.g., prompts) of a generative\\nAI model and hope that the model can generate better output with or without a\\nspecific target task [92]. It is also referred to as the “Retrieve-then-Read” framework\\nthat has been used in reading comprehension and text summarization before LLMs\\nhit the world [93]. Given an input (could be a query or a specific task instruction),\\nwe first retrieve relevant information (usually entities, passages, or documents) from\\nan external corpus or previous inputs (e.g., the memory of an agent [94, 95]) with a\\nretrieval system. Then, we craft a input prompt with the retrieval results and feed it\\nto the LLM. The LLM will generate the final response based on the input request and\\n11\\nthe retrieved information. This paradigm has already been proven to be effective in\\nmultiple IA tasks such as question answering [85].\\nBecause LLMs are purely used as black-box tools to process the retrieved doc-\\numents and input request in naive RAG, existing studies on this direction mainly\\nfocus on the development of better retrieval systems and prompt design for RAG. The\\nstudies on retrieval systems, unsurprisingly, are highly similar to those in IR, which\\ninvolve indexing, query processing, first-stage retrieval, re-ranking, etc. These topics\\nand system components have already been studied in the IR community for more than\\nfive decades. Perhaps the most notable difference is that recent studies on naive RAG\\noften prefer the use of neural retrieval models (e.g., dense retrieval models [96]) over\\ntraditional term-matching models (e.g., BM25 [97]). An important reason behind this\\nis that neural retrieval models share similar theoretical background and model struc-\\ntures with LLMs. This makes joint optimization possible in modern RAG systems,\\nwhich we discuss in Section 2.1.3.\\nThe design of input prompts with retrieval results, on the other hand, is relatively\\nmore under-explored before the rise of LLMs. It has been well recognized that prompt\\nformats, even when the contents are same, could significantly affect the performance of\\nLLMs. How to feed retrieval results effectively into the prompts of LLMs for RAG has\\nthus attracted a lot of attention recently [93, 98, 99]. Studies have found that LLMs\\nexhibit significant position bias over the input result sequences [100, 101], and has\\ndifferent perspectives on relevance with human experts [102]. Since prompts are the\\nmain interaction interface between retrieval and generation, their design principles and\\ndownstream effects on naive RAG are of great value both in research and real-world\\napplications. Particularly, how to craft effective RAG prompts automatically could be\\na fruitful direction to explore. Existing studies have shown that high-quality prompt\\nwriters can be automatically learned based on downstream task performance and user\\nlogs in image generation [53], and it is widely believed that similar techniques have\\nalso been used in popular LLM chatbots [103]. Yet, how to do this for RAG remains\\nto be a question to be answered.\\n2.1.2 Modular RAG\\nIn contrast to naive RAG methods, modular RAG treats retrieval systems as func-\\ntional modules to support LLMs [104]. While some works view this retrieval module\\nas one type of many tools that can be learned and used by LLMs [105], it is widely\\nacknowledged that retrieval systems possesses a irreplaceable position in modern\\nLLM applications due to its diverse nature and significant importance [84]. Broadly\\nspeaking, existing studies on using retrieval systems as functional modules for LLM\\ngeneration mainly focus on the three “W” questions, namely when to retrieve,what to\\nretrieve, and where to retrieve.\\nThe question of when to retrieve refers to the timing of functional call for retrieval\\nsystems. In contrast to LLMs that directly create responses based on their internal\\nparameter space without explicit evidence grounding, retrieval systems produces reli-\\nable and explainable information directly by searching external corpus. From this\\nperspective, the best timing to call the retrieval system is when LLMs start to hallu-\\ncinate or produce wrong results. Yet, identifying such timing is difficult because we\\n12\\nneither know the correct answers in advance or understand the internal mechanism of\\nLLMs (at least of today) [106]. One naive yet effective method is to retrieve supporting\\nevidence for LLM inference with a fixed time interval, such as every fixed number of\\ngenerated tokens[107, 108] or every sentence [109]. More advanced paradigms involve\\nthe analyze of knowledge boundary [110] and the estimation of prediction uncertainty\\nin LLMs [106, 111]. Theoretically speaking, since the study of when to retrieve shares\\nsimilar motivations and foundations with the study of hallucination detection, exist-\\ning studies on LLM hallucination [112, 113] could provide important inspiration for\\nresearch on this topic. Promising directions including better fact checking systems\\nfor LLMs [114] and more investigations on how to characterize the confidence and\\nuncertainty of LLM predictions based on both external behavior and internal state\\nanalysis [111].\\nThe question of what to retrieve focuses on analysis the intents and information\\nneeds of LLMs in inference. LLMs often need the help of different tools and systems to\\nfinish different tasks [105]. However, in contrast to other tools widely studied in tool\\nlearning, retrieval itself is a complicated systems with dynamic and free-form inputs,\\ndata collections, and outputs. Therefore, understanding what exactly is needed by\\nLLMs and how to formulate it in the language of retrieval systems is an important\\nproblem. Most existing studies on RAG naively use the whole or local context of LLM\\ninference as the queries to retrieval systems and assume that these context contain\\nenough information to guide retrieval [90]. A slightly better solution is to use the terms\\nthat LLMs have low confidence to formulate queries since uncertain tokens represent\\ncases where LLMs have limited knowledge to generate responses and thus need more\\ninformation [106]. As long studied in the IR community, the formulation of an effective\\nquery requires deep understanding of the user’s intent, and many of the important\\ncontext information behind a user intent is not explicitly expressed in the words they\\nwrote [115]. Therefore, a more theoretically principled method to answer what to\\nretrieve in RAG is to analyze the internal state of LLMs and infer their information\\nneeds directly. For example, Su et al. [111] directly formulate queries based on the\\ninternal attention distribution of LLMs (Figure 2) and improve the performance of\\nRAG for nearly 20% on several benchmark datasets without changing the retrieval\\nsystem. This demonstrates the potential of future studies on this direction.\\nWhere to retrieve refers to the question of how to identify the correct information\\nsources for RAG. Studies on this direction is particularly related to the research on\\nmulti-source retrieval [116] and tool learning [105]. To answer different requests related\\nto the use of information collected from different databases or data collections, LLMs\\nneed to learn how to interact with each information sources effectively and efficiently.\\nThe studies of tool learning focus on teaching LLMs to use tools according to the\\ncontext, and retrieval systems are usually considered as one type of tools to use.\\nHowever, retrieval itself could be a complicated problem when we possess multiple data\\ncollections with different characteristics. In search engines, information sources are\\nbroadly categorized based on their modality, and we usually build separate systems for\\neach of them (e.g., the “Images”, “News”, “Videos” tabs on Google). While commercial\\nsearch engines may aggregate results from different sources into a single page, the\\nultimate search engine result page (SERP) shown to users are just a list of results and\\n13\\nFig. 2 Su et al. [111] generate queries for RAG based on the internal attention distribution of LLMs.\\nit’s up to the users to decide which they want to see and how to use these results for\\ndownstream applications. In contrast, when using LLMs, users often request LLMs\\nto directly answer their question instead of listing a couple of candidates [117, 118],\\nso it’s the job of LLMs to decide where to retrieve the information given the current\\ncontext. While the studies of how to navigate user queries to search indexes built from\\ndifferent information sources have been widely studied in the IR community [119–\\n122], how to do it for RAG with modern generative AI models is, to the best of our\\nknowledge, still underexplored. Existing literature on RAG mostly works on a single\\nretrieval collection (usually a text corpus), but it’s obvious that no single collection\\ncan satisfy the needs of LLMs in different tasks. For instance, when writing a legal\\n14\\ncase document, the judge needs to collect and organize information from evidences,\\ncomplaints, counterclaims, court records, as well as legal articles and previous cases.\\nHow to navigate the generation model to retrieve and integrate information from\\ndifferent sources jointly for downstream applications is a practical and potentially\\nfruitful research question for RAG.\\n2.1.3 Optimization of Retrieval and Generation\\nAs discussed in several RAG surveys [84, 90], the optimization of RAG systems usually\\ninvolves the optimization of three components, i.e., the retriever, the generator, and the\\naugmentation method. If we further step back and look at the high-level goals of RAG\\noptimization, we could also categorize it based on how we evaluate the RAG system,\\nnamely the evaluation from the perspectives of retrievers, generators, or the joint\\nsystems. The evaluation from the retriever perspectives is not particularly different\\nfrom existing studies on ranking evaluation. The underlining assumption of this is\\nthat, once the LLMs are fed with the passages or documents that contain the correct\\ninformation, they should be able to produce the correct answers directly. Therefore,\\nthe evaluation and optimization of a RAG system could downgrade to the evaluation\\nand optimization of a classic retrieval/ranking systems, to where most existing works\\non dense retrieval and LTR could be applied [123, 124]. Yet, there are still differences\\nbetween RAG and traditional retrieval tasks as the queries are no long issued by users.\\nHow to formulate queries efficiently and effectively from LLMs for the retriever is\\nworthy research question, and studies on this direction has already shown potentials\\nin improving the overall quality of RAG systems [111].\\nFrom the perspective of generators, RAG evaluation and optimization focus more\\non improving the robustness and effectiveness of LLM generation based on a fixed set\\nof retrieval results [108]. This often means extra training or fine-tuning on LLMs to\\nimprove their fundamental ability in information processing. For example, retrieved\\ndocuments could be lengthy, and LLMs are usually not good at processing long input\\ncontext [101]. Therefore, how to design efficient LLMs that can take long context\\ninputs efficiently and effectively has been a popular research problem that have been\\nwidely studied by researchers from both academia and industry [100]. We have seen\\nmany companies show off their models based on how many input tokens they can\\nprocess in one request. In addition, since retrieval results are fed as a part of the\\nLLM inputs, whether the LLMs can generate the response based on the retrieved\\ndocuments instead of their internal knowledge could be seen as a special type of\\ninstruction-following ability. Studies have been conducted to teach LLMs to utilize\\nretrieval results faithfully and constantly in RAG systems [125] On the other hand,\\nfactors such as irrelevant results and ranking perturbations are well acknowledged to\\nbe harmful for the performance of generators in RAG, so there are also studies that\\ntry to improve the robustness of LLMs from the perspective of RAG. For example,\\nZhang et al. [126] proposes to fine tune LLMs with the presence of retrieval results\\n(i.e., retrieval augmented fine tuning) so that LLMs can learn the domain-specific\\nknowledge introduced by the retriever and improve their robustness against potential\\ndistracting information from retrieval.\\n15\\nFrom the perspective of augmentation methods, existing research mostly focuses\\non the joint optimization of RAG system as a whole. In other words, the loss functions\\nof RAG optimization should be built from the performance metrics of downstream\\ntasks directly. While this paradigm is appealing, it often has strict requirements on\\nthe design of RAG systems. Particularly, it’s difficult to apply such joint optimiza-\\ntion algorithms on a RAG system in which retrievers and generators are loosely\\nconnected through prompts constructed from discrete retrieval results. While rein-\\nforcement learning could solve the problem in theory, its empirical performance when\\nbeing used as the solo optimization algorithms for ranking systems is still not satisfying\\nat this point [127]. If you already have a good retriever and only conduct fine-tuning\\nwith a fixed LLM, then it may work [128], but this still doesn’t look like a perfect\\nsolution because reinforcement learning usually subject to large variance in practice.\\nTo the best of our knowledge, how to directly connect the training of retrievers with\\nthe auto-regressive loss of the generators in RAG is still an open question. Answering\\nthis question requires us to go deep into the structure of generative AI models and\\nretrieval models, and develop new model structures that can take advantages from\\nstudies on both sides.\\n2.1.4 Retrieval Planning and Composite Information Needs\\nAs discussed above, the initial motivation behind the studies of RAG mostly focuses\\non using the power of retrieval systems to improve the quality of responses generated\\nby LLMs in terms of reliability and informativeness. While it is widely acknowledged\\nthat problems such as hallucination and high computation cost in supervised fine-\\ntuning will continue to be significant for generative AI models in a short period of\\ntime, there are also concerns, especially from the IR community, that retrieval could\\nbecome less important with the rapid evolution of LLMs [129]. In fact, ChatGPT\\nhas already shown similar accuracy and better user satisfaction on factoid question\\nanswering than traditional web search engines [1]. However, the rise of generative AI\\nmodels also brings brand new opportunities for IR. One of them is the possibility of\\nmoving from SERPs that simply list result candidates to a real information agent that\\nsolve complicated tasks with composite information needs.\\nToday, most people treat IR systems as unit information solvers. Despite of their\\nactual task characteristics, users first decompose their goals into a couple of unit\\ninformation need (usually expressed with separate queries), and then issue them one by\\none to search engines or recommendation systems to find the corresponding answers.\\nAn important reason behind the popularity of this paradigm is that, at least of today,\\nIR systems are not capable of doing complicated information tasks with composite\\nneeds and multi-step planning. For example, we can use a search engine to find a survey\\non RAG by searching ”survey of RAG”, but cannot write such a survey directly by\\nretrieving and analyzing papers from publication collections. The job of information\\nneed decomposition and retrieval planning has always been human’s.\\nFortunately, with the help of generative AI models like LLMs, it is now possi-\\nble to push the boundary of IR systems and tackle such advanced information tasks\\nfor users. Composite retrieval is not a new concept in IR [130], but previous studies\\nrefer to the phrase as retrieval paradigms that cluster results from multiple sources\\n16\\nand show them in groups for specific user queries [131]. While this represents one\\ntype of composite needs, it is relatively simple as the target user queries usually are\\nmostly topic-specific and keyword-based. Complicated information tasks such as sur-\\nvey generation and professional document writing often involve multi-step planning\\nand multi-round interactions between the retrieval results and response generation. To\\nbuild powerful IR systems or agents that can solve such composite information tasks,\\nwe need to construct collaborative systems that deeply connect the retrieval, plan-\\nning, and generation. For instance, we need to conduct generation-oriented retrieval\\noptimization to build retrieval framework and model interfaces for downstream task\\nplanner and response generators; we also need to design retrieval-oriented generation\\nmodels that can decompose information needs, navigate the retrieval process, gather\\ninformation from multiple sources to generate the final results. Research on these direc-\\ntions could be fruitful and significantly extend the scope of IR in the era of generative\\nAI.\\n2.2 Corpus Modeling and Understanding\\nIn contrast to using RAG, another line of studies try to use generative AI models\\nto replace traditional retrieval systems. Directly answering user’s information need\\ninstead of showing ten blue links has long been an important goal for the development\\nof intelligent IR systems [132]. With the rise of LLMs, such vision is now achievable\\nin a significant extent. For example, LLM-based chatbots like ChatGPT can answer\\nmultiple types of user queries with direct answers [118]. Metzler et al. [133] has dis-\\ncussed several paradigms in which pre-trained language models can help IR systems\\nanswer user’s information needs directly without listing references. The intuition is to\\nuse neural network based language models to store the corpus knowledge in parameter\\nspace and pull relevant answers or information directly from it based on user’s queries.\\nDepending on how the problem is formulated, several research directions have emerged.\\nSpecifically, in this section, we discuss two of them, namely generative retrieval and\\ndomain-specific modeling.\\n2.2.1 Generative Retrieval\\nThe idea of Generative Retrieval comes from the idea of differentiable index proposed\\nby Metzler et al. [133]. The original name used in the paper was Model-based IR, but\\nafter the rise of generative AI models, some researchers start to refer to studies on\\nthis direction as generative retrieval (GR). The core idea of GR is two-fold, i.e., the\\ndifferentiable index and the generation of doc IDs.\\nInspired by the superior performance of pre-trained language models, particularly\\nBERT [8] and GPT [1], generative retrieval wants to explore the possibility of replacing\\ntraditional term-based index (e.g., inverted index) in retrieval systems with large-\\nscale neural networks. In contrast to dense retrieval models that build neural encoders\\nto project documents to latent semantic spaces and build explicit indexes based on\\ndocument vectors, GR tries to build implicit indexes in the parameter space of neural\\nnetworks. For instance, DSI and its variations [134–137] have tried to train pretrained\\nlanguage models on the target corpus directly and then treat the model’s parameter\\n17\\nas a “index” of the corpus. Studies on this direction argue that, by training the neural\\nmodels to encode the whole corpus, documents and information would be implicitly\\nstored in the parameters of the models, and this parameter-based indexes have better\\nstorage efficiency than traditional term-based or vector-based indexes [135]. They also\\nargue that such paradigm can unify the multi-stage retrieval pipeline so that indexes\\ncan be trained directly for the final retrieval objectives. However, storing raw document\\ncontent directly in limited parameter spaces often lead to significant information loss\\n(which is reflected in the suboptimal retrieval performance of GR models [138]), and\\nusing model parameters as indexes make the whole system uncontrollable by both\\nsystem developers and users. While the former could be alleviated by using large-scale\\nmodels, the later is still an unresolved problem for GR. For example, it’s difficult, if\\nnot impossible, to remove or update a document indexed in the parameter space when\\nwe don’t know what exactly each parameter do in the neural models. Considering\\nthat dense retrieval models built with product quantization and inverted file systems\\ncan achieve state-of-the-art retrieval performance with similar latency and less storage\\nthan term-based models with inverted indexes [139], whether the idea of differentiable\\nindexes in GR worth its price is still a controversial question.\\nAnother important characteristic of GR models is to retrieve documents by gen-\\nerating sequences of doc IDs through auto-regression. Because documents are stored\\nimplicitly in model parameters, to actually retrieve a real document, GR models use\\nuser’s queries as prompts to generate document IDs, which usually consist of a couple\\nof special tokens, that exclusively identify each relevant document. Since the birth of\\nGR, a variety types of document IDs have been proposed, which can be broadly catego-\\nrized as IDs with explicit tokens [134–136] and IDs with implicit tokens [137, 140, 141].\\nGR models with explicit ID tokens try to label each document with sequences of real\\nterms that have semantic or numerical meanings. Examples including keyword-based\\ndoc IDs and tree-based doc IDs [135]. Compared to vectors in dense retrieval, these\\nmethods have less flexibility and capability in document modeling as they discretize\\ndocument semantic meanings with limited number of tokens, and their retrieval per-\\nformance is usually poor [140]. However, they have better explanability than other\\nneural retrieval models because their doc ID tokens are constructed from real words\\nor document clusters. To avoid the theoretical limitation of explicit token IDs and\\ngrant GR models with the same modeling capacity of dense retrieval models, several\\nstudies have proposed to build implicit token IDs with latent vectors [137, 140, 141].\\nThe idea is to represent each document with a sequence of latent vectors so that fine-\\ngrained semantic information would not be lost. These types of GR models are highly\\nsimilar to existing dense retrieval models since both of them represent each document\\nwith latent vectors. The major difference is that the former uses a sequence of vectors\\nfrom a learned codebook constructed in training, while the later builds separate vec-\\ntors for each document directly from their raw content. Wu et al. [142] have proved\\nthat the GR models with implicit tokens are equal to a multi-vector dense retrieval\\nmodels in theory. Also, the use of a learned codebook for implicit token vectors is\\ntheoretically the same with a dense retrieval system that uses cluster-based product\\nquantization [139, 143]. Therefore, the performance upper bound of GR (with implicit\\ntokens) and dense retrieval is the same in theory. While some believe that GR models\\n18\\ncould have lower latency as they don’t need to search among millions of documents\\non the fly, this is a questionable argument because the inference of a large-scale neu-\\nral model is usually much slower than a vector-based search on distributed systems.\\nAlso, the maintenance of information in a neural model is much more difficult than\\na vector-based database. Perhaps the future potential of GR does not lay in retrieval\\neffectiveness or efficiency, but some other perspectives such as explainability.\\n2.2.2 Domain-specific Modeling\\nLLMs, particularly those with instruction tuning, can response to user’s queries\\ndirectly. This exactly matches the initiative of a long-standing vision of IR systems\\nto directly answer user’s need without listing a couple of documents [133]. Therefore,\\never since the rise of ChatGPT, there has been serious discussion on whether LLMs\\nare future seach engines in practice [129]. Yet, apart from the hallucination problem\\ndiscussed in previous sections, there are other challenges that prevent generative AI\\nmodels like LLMs to serve as a major information accessing tool for modern users.\\nOne of them is how to teach LLMs to understand and use knowledge from external\\ncorpus not included in their initial training process. If we treat each external corpus\\nas a domain-specific dataset, then the studies on this direction is essentially the same\\nwith the construction of domain-specific LLMs. While RAG can help LLMs adapt to\\nnew domains quickly, their performance is limited when the understanding of input\\ndocuments from the external corpus requires domain knowledge that the LLMs do not\\npossess in advance [83].\\nTo solve the above problem and build usable IA systems with LLMs on domain-\\nspecific data, one of the most popular method is to conduct continue pre-training or\\nsupervised fine-tuning of LLMs on the target domain corpus. The idea is to apply\\nsimilar training strategies used in model pre-training on the new corpus so that LLMs\\ncan better capture knowledge in the new domain. Example studies on this direction\\ninclude techniques on data selection [82] and tokenizers adaptation [144] that directly\\nuse the target corpus to train LLMs. Many domain-specific LLMs have been developed,\\ninclude legal LLMs, financial LLMs, etc. [145–147] The continue pre-training of LLMs\\non external corpus has been shown to be effective on many domain-specific tasks such\\nas domain QA and text generation. However, modeling external corpus through this\\nmethod may not be preferred in practice when we don’t have enough computation\\nresources to train LLMs or can’t access the parameters of them. Also, till the end\\nof the today, the internal knowledge structure and learning mechanism of LLMs are\\nstill unknown, and applying naive continue pre-training algorithms on external corpus\\ncould hurt the performance of LLMs in unexpected way. Therefore, researchers have\\ndesigned several knowledge editing techniques on LLMs to explore the possibility of\\ninjecting knowledge with no or low cost on the general effectiveness of LLMs [148, 149].\\nStudies on this direction is still in an early stage as most existing methods only work\\non fixed and limited updating rules and knowledge entity triples [150], but it could be\\nfruitful in future since domain adaption and external corpus modeling is a wide need\\nof LLM applications in practice.\\nBesides continue pre-training, another paradigm to model external corpus and\\ndomain knowledge is to build separate language models for each corpus and combine\\n19\\nthem with the large general LLMs to form a collaborative system. The intuition behind\\nthis is relevant to the idea of LLM agents where each LLM could serve as different roles\\nin the system to accomplish tasks together. It is widely acknowledged that the phe-\\nnomenon of emergent abilities only present in large-scale models [29], but the training\\ncost of such models (e.g., GPT-4 [1]) is usually prohibitive to research institutes and\\nsmall companies, even with parameter efficient algorithms [151] Inspired by the supe-\\nrior instruction following ability of LLMs, researchers have explored the possibility\\nof building small models for external corpus modeling and use them to communicate\\ndomain-specific knowledge to large general LMs [83]. In other words, the small mod-\\nels can serve as domain knowledge “ consultants” and the large general models can\\nserve as the decision makers that finish domain-specific tasks based on the guidance\\nof the small models. Experiments have shown that such paradigm can improve black-\\nbox LLMs’ performance on domain-specific tasks with low cost and high flexibility.\\nWhile the overall idea of prompt general LLMs with domain-specific prompts is sim-\\nilar to the framework of RAG, building an actual LM for corpus modeling enable us\\nto capture implicit domain knowledge (e.g., the fine-grained differences between law\\narticles [152]) and potentially save tokens in prompts. There are concerns on whether\\nthis paradigm is still worthy when we have more powerful LLMs that include more\\ndomain-specific data in training. However, since many users prefer to keep their data\\nprivate to themselves due to multiple safety and privacy concerns, this paradigm and\\nRAG could continue to be appealing in practice.\\n3 Summary and Future Directions\\nIn this chapter, we introduce the foundations and applications of generative AI models\\nin information accessing. Instead of analyzing how generative AI models like LLMs\\ncould improve the existing modules of search engines and recommendation systems, we\\nfocus on how the they could revolutionize information access with new methodologies\\nand system design. Particularly, we discuss two new paradigms brought by generaive\\nAI models, namely information generation and information synthesis.\\nInformation generation refers to the scenarios where users can use generative AI\\nmodels to create information that directly satisfies their information needs. Here, we\\ndelved into the core components of generative models, including model architectures\\n(with a focus on Transformers and their improvements), scaling laws, and training\\nmethodologies. We examined the debates surrounding continual model scaling, the\\nimportance of prompt optimization, and the extension of these models to multi-modal\\napplications for information access.\\nInformation synthesis refers to the paradigm that utilizes the superior instruction-\\nfollowing and logic-reasoning ability of LLMs to aggregate and synthesize existing\\ninformation. We extensively discuss one of the most representative techniques, i.e.,\\nRetrieval Augmented Generation (RAG), on this direction, and introduce various\\napproaches from naive implementations to more sophisticated modular systems. We\\ndescribe the challenges and opportunities in optimizing RAG systems, highlighting\\nthe need for joint retrieval-generation optimization and the potential of several rele-\\nvant research directions such as composite retrieval with planning. Besides RAG, we\\n20\\nalso discuss some alternative paradigms that use generative AI models to model cor-\\npus knowledge directly, such as generative retrieval, which aims to replace traditional\\nindexing methods with neural network-based approaches, and domain-specific model\\ntraining, which conducts continue pre-training or fine-tuning on LLMs with the tar-\\nget corpus. We discussed the potential and limitations of these approaches, including\\nissues of system controllability and cost efficiency.\\nOverall, research on how generative AI models could reshape modern information\\naccess systems is still at an early stage today. As discussed above, existing studies on\\ninformation generation and information synthesis either focus on simple information\\ntasks (such as writing a poem, answering a factoid question, etc.) or reply on simple\\nsystem design (e.g., feeding all documents to LLMs as prompts) that obviously can-\\nnot fully exploit the power of modern retrieval and generation models. Therefore, we\\nbelieve that there are two major directions worth exploring in the next couple of years\\n(at least). The first one is to move from simple and unit information retrieval tasks\\n(e.g., factoid QA) to more complicated information tasks that used to be “impossible”\\nfor modern IR systems. Examples include retrieval with composite needs (e.g., ”help\\nme plan a wedding in Amherst, MA”) or tasks that requires planning and multiple\\nrounds of retrieval and generations (e.g., ”write a survey on RAG”). These tasks used\\nto require human experts to decompose the needs and conduct retrieval, analysis, and\\nresult aggregations. With the help of generative AI, accomplishing them automatically\\nwith machines is now possible. The second direction is to explore better techniques\\nto communicate, collaborate, or even unify retrieval and generation systems for infor-\\nmation accessing. While the studies of RAG have attracted considerable attention,\\nexisting works mostly use retrieval systems as plug-in tools for LLMs without digging\\ninto their internal connections and differences. Examples such as how to understand\\nthe information needs of LLMs, how to communicate the retrieved results to LLMs,\\nand how to optimize generators for retrieval and retriever for generation are all impor-\\ntant yet underexplored research topics. There are many questions related to each of\\nthese topics that worth detailed investigation, including the design of new training\\nparadigms, the development of agent-like system frameworks, potential problems and\\nbias introduced by off-policy and on-policy training for the joint system, etc.\\nWhen ChatGPT first arrives, there are people from the IR community worried\\nthat such generative AI models could overthrow all existing IR systems and crush\\neverything in the field [129], as it has almost happened in NLP. Interestingly, in sim-\\nulated social experiments on human-AI competitions, Yao et al. [153] find that, if\\nhuman producers don’t extend their capacities with the help of generative AI, they\\nwill eventually be “replaced” by AI. From this perspective, the future of IR research\\nin the era of generative AI lies in how to extend the scope of IR with generative AI\\nmodels to finish more complicated information tasks and develop more general system\\narchitectures that not just retrieve a list of document, but perform more sophisticated\\ninformation processing and planning.\\nReferences\\n[1] OpenAI: GPT-4 technical report. CoRR abs/2303.08774 (2023) https://doi.\\n21\\norg/10.48550/ARXIV.2303.08774 2303.08774\\n[2] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,\\nZhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R.,\\nLi, Y., Tang, X., Liu, Z., Liu, P., Nie, J., Wen, J.: A survey of large language\\nmodels. CoRR abs/2303.18223 (2023) https://doi.org/10.48550/ARXIV.2303.\\n18223 2303.18223\\n[3] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,\\nRozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\\n[4] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\\nKaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\\nU.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)\\nAdvances in Neural Information Processing Systems, vol. 30. Curran Associates,\\nInc., ??? (2017)\\n[5] Schuster, M., Paliwal, K.K.: Bidirectional recurrent neural networks. IEEE\\ntransactions on Signal Processing 45(11), 2673–2681 (1997)\\n[6] Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y.,\\nZheng, W., Xia, X., et al.: Glm-130b: An open bilingual pre-trained model. arXiv\\npreprint arXiv:2210.02414 (2022)\\n[7] Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ili´c, S., Hesslow, D., Castagn´e, R.,\\nLuccioni, A.S., Yvon, F., Gall´e, M., et al.: Bloom: A 176b-parameter open-access\\nmultilingual language model (2023)\\n[8] Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidi-\\nrectional transformers for language understanding. In: NAACL-HLT (1), pp.\\n4171–4186. Association for Computational Linguistics, ??? (2019)\\n[9] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,\\nY., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified\\ntext-to-text transformer. J. Mach. Learn. Res. 21, 140–114067 (2020)\\n[10] Press, O., Smith, N.A., Lewis, M.: Train short, test long: Attention with lin-\\near biases enables input length extrapolation. arXiv preprint arXiv:2108.12409\\n(2021)\\n[11] Su, J., Lu, Y., Pan, S., Wen, B., RoFormer, Y.L.: Enhanced transformer with\\nrotary position embedding., 2021. DOI: https://doi. org/10.1016/j. neucom\\n(2023)\\n[12] Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He,\\nH., Leahy, C., McDonell, K., Phang, J., et al.: Gpt-neox-20b: An open-source\\n22\\nautoregressive language model. arXiv preprint arXiv:2204.06745 (2022)\\n[13] Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with\\nsparse transformers. arXiv preprint arXiv:1904.10509 (2019)\\n[14] Kitaev, N., Kaiser,  L., Levskaya, A.: Reformer: The efficient transformer. arXiv\\npreprint arXiv:2001.04451 (2020)\\n[15] Munkhdalai, T., Faruqui, M., Gopal, S.: Leave no context behind: Efficient infi-\\nnite context transformers with infini-attention. arXiv preprint arXiv:2404.07143\\n(2024)\\n[16] Grave, E., Joulin, A., Usunier, N.: Improving neural language models with a\\ncontinuous cache. arXiv preprint arXiv:1612.04426 (2016)\\n[17] Izacard, G., Grave, E.: Leveraging Passage Retrieval with Generative Models for\\nOpen Domain Question Answering. arXiv (2020). https://arxiv.org/abs/2007.\\n0128\\n[18] Shazeer, N.: Fast transformer decoding: One write-head is all you need. arXiv\\npreprint arXiv:1911.02150 (2019)\\n[19] Ainslie, J., Lee-Thorp, J., Jong, M., Zemlyanskiy, Y., Lebr´on, F., Sanghai,\\nS.: Gqa: Training generalized multi-query transformer models from multi-head\\ncheckpoints. arXiv preprint arXiv:2305.13245 (2023)\\n[20] DeepSeek-AI: DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-\\nExperts Language Model (2024)\\n[21] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y.,\\nWang, L., Liu, T.: On layer normalization in the transformer architecture. In:\\nInternational Conference on Machine Learning, pp. 10524–10533 (2020). PMLR\\n[22] Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou,\\nX., Shao, Z., Yang, H., et al.: Cogview: Mastering text-to-image generation via\\ntransformers. Advances in Neural Information Processing Systems 34, 19822–\\n19835 (2021)\\n[23] Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., Wei, F.: Deepnet: Scal-\\ning transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence (2024)\\n[24] Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R.,\\nGray, S., Radford, A., Wu, J., Amodei, D.: Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 (2020)\\n[25] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,\\n23\\nE., Casas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training compute-\\noptimal large language models. arXiv preprint arXiv:2203.15556 (2022)\\n[26] Ye, J., Liu, P., Sun, T., Zhou, Y., Zhan, J., Qiu, X.: Data mixing laws: Optimizing\\ndata mixtures by predicting language modeling performance. arXiv preprint\\narXiv:2403.16952 (2024)\\n[27] Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun,\\nH., Brown, T.B., Dhariwal, P., Gray, S., et al.: Scaling laws for autoregressive\\ngenerative modeling. arXiv preprint arXiv:2010.14701 (2020)\\n[28] Fang, Y., Zhan, J., Ai, Q., Mao, J., Su, W., Chen, J., Liu, Y.: Scal-\\ning laws for dense retrieval. In: Proceedings of the 47th International\\nACM\\nSIGIR\\nConference\\non\\nResearch\\nand\\nDevelopment\\nin\\nInformation\\nRetrieval. SIGIR ’24, pp. 1339–1349. Association for Computing Machin-\\nery, New York, NY, USA (2024). https://doi.org/10.1145/3626772.3657743 .\\nhttps://doi.org/10.1145/3626772.3657743\\n[29] Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama,\\nD., Bosma, M., Zhou, D., Metzler, D., et al.: Emergent abilities of large language\\nmodels. arXiv preprint arXiv:2206.07682 (2022)\\n[30] Du, Z., Zeng, A., Dong, Y., Tang, J.: Understanding emergent abilities of\\nlanguage models from the loss perspective. arXiv preprint arXiv:2403.15796\\n(2024)\\n[31] Power, A., Burda, Y., Edwards, H., Babuschkin, I., Misra, V.: Grokking: Gen-\\neralization beyond overfitting on small algorithmic datasets. arXiv preprint\\narXiv:2201.02177 (2022)\\n[32] Schaeffer, R., Miranda, B., Koyejo, S.: Are emergent abilities of large language\\nmodels a mirage? In: Proceedings of the 37th International Conference on Neural\\nInformation Processing Systems. NIPS ’23, pp. 1–13. Curran Associates Inc.,\\nRed Hook, NY, USA (2024)\\n[33] McKenzie, I.R., Lyzhov, A., Pieler, M., Parrish, A., Mueller, A., Prabhu, A.,\\nMcLean, E., Kirtland, A., Ross, A., Liu, A., et al.: Inverse scaling: When bigger\\nisn’t better. arXiv preprint arXiv:2306.09479 (2023)\\n[34] Mei, K., Tu, Z., Delbracio, M., Talebi, H., Patel, V.M., Milanfar, P.: Bigger is\\nnot always better: Scaling properties of latent diffusion models. arXiv preprint\\narXiv:2404.01367 (2024)\\n[35] Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang,\\nY., Zhao, W., et al.: Minicpm: Unveiling the potential of small language models\\nwith scalable training strategies. arXiv preprint arXiv:2404.06395 (2024)\\n24\\n[36] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.:\\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8), 9\\n(2019)\\n[37] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,\\nDiab, M., Li, X., Lin, X.V., et al.: Opt: Open pre-trained transformer language\\nmodels. arXiv preprint arXiv:2205.01068 (2022)\\n[38] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,\\nBarham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling lan-\\nguage modeling with pathways. Journal of Machine Learning Research 24(240),\\n1–113 (2023)\\n[39] Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C.T., Del Giorno, A., Gopi, S.,\\nJavaheripi, M., Kauffmann, P., Rosa, G., Saarikivi, O., et al.: Textbooks are all\\nyou need. arXiv preprint arXiv:2306.11644 (2023)\\n[40] Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D.,\\nWang, D., Yan, D., et al.: Baichuan 2: Open large-scale language models. arXiv\\npreprint arXiv:2309.10305 (2023)\\n[41] Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K.,\\nDu, Q., Fu, Z., et al.: Deepseek llm: Scaling open-source language models with\\nlongtermism. arXiv preprint arXiv:2401.02954 (2024)\\n[42] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,\\nX., Dehghani, M., Brahma, S., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen,\\nX., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A.W., Zhao, V.,\\nHuang, Y., Dai, A.M., Yu, H., Petrov, S., Chi, E.H.-h., Dean, J., Devlin, J.,\\nRoberts, A., Zhou, D., Le, Q.V., Wei, J.: Scaling instruction-finetuned language\\nmodels. ArXiv abs/2210.11416 (2022)\\n[43] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\\ninstructions with human feedback. Advances in neural information processing\\nsystems 35, 27730–27744 (2022)\\n[44] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal Policy\\nOptimization Algorithms (2017)\\n[45] Rafailov, R., Sharma, A., Mitchell, E., Manning, C.D., Ermon, S., Finn, C.:\\nDirect preference optimization: Your language model is secretly a reward model.\\nIn: Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., Levine, S. (eds.)\\nAdvances in Neural Information Processing Systems, vol. 36, pp. 53728–53741.\\nCurran Associates, Inc., ??? (2023)\\n[46] Xu, S., Fu, W., Gao, J., Ye, W., Liu, W., Mei, Z., Wang, G., Yu, C., Wu, Y.: Is\\n25\\ndpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint\\narXiv:2404.10719 (2024)\\n[47] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language\\nprocessing. ACM Computing Surveys 55(9), 1–35 (2023)\\n[48] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\\nAdvances in neural information processing systems 35, 24824–24837 (2022)\\n[49] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.L., Cao, Y., Narasimhan,\\nK.: Tree of thoughts: deliberate problem solving with large language models.\\nIn: Proceedings of the 37th International Conference on Neural Information\\nProcessing Systems. NIPS ’23, pp. 1–14. Curran Associates Inc., Red Hook, NY,\\nUSA (2024)\\n[50] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Zhou, D.: Self-consistency\\nimproves chain of thought reasoning in language models. In: 11th International\\nConference on Learning Representations, ICLR 2023, pp. 1–15 (2023)\\n[51] Zhou, Y., Muresanu, A.I., Han, Z., Paster, K., Pitis, S., Chan, H., Ba,\\nJ.: Large language models are human-level prompt engineers. arXiv preprint\\narXiv:2211.01910 (2022)\\n[52] Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large\\nlanguage models as optimizers. arXiv preprint arXiv:2309.03409 (2023)\\n[53] Zhan, J., Ai, Q., Liu, Y., Chen, J., Ma, S.: Capability-aware prompt refor-\\nmulation learning for text-to-image generation. In: Proceedings of the 47th\\nInternational ACM SIGIR Conference on Research and Development in Infor-\\nmation Retrieval. SIGIR ’24, pp. 2145–2155. Association for Computing Machin-\\nery, New York, NY, USA (2024). https://doi.org/10.1145/3626772.3657787 .\\nhttps://doi.org/10.1145/3626772.3657787\\n[54] Zhan, J., Ai, Q., Liu, Y., Pan, Y., Yao, T., Mao, J., Ma, S., Mei,\\nT.: Prompt refinement with image pivot for text-to-image generation. In:\\nKu, L.-W., Martins, A., Srikumar, V. (eds.) Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pp. 941–954. Association for Computational Linguis-\\ntics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.acl-long.53 .\\nhttps://aclanthology.org/2024.acl-long.53/\\n[55] Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language tasks. Advances in neural\\ninformation processing systems 32 (2019)\\n26\\n[56] Chen, Y.-C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:\\nUniter: Universal image-text representation learning. In: European Conference\\non Computer Vision, pp. 104–120 (2020). Springer\\n[57] Huang, Z., Zeng, Z., Liu, B., Fu, D., Fu, J.: Pixel-bert: Aligning image pixels\\nwith text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849\\n(2020)\\n[58] Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,\\nJ., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a sim-\\nple sequence-to-sequence learning framework. In: International Conference on\\nMachine Learning, pp. 23318–23340 (2022). PMLR\\n[59] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,\\nMensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model\\nfor few-shot learning. Advances in neural information processing systems 35,\\n23716–23736 (2022)\\n[60] Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,\\nL., Song, X., et al.: Cogvlm: Visual expert for pretrained language models. arXiv\\npreprint arXiv:2311.03079 (2023)\\n[61] Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-\\ntraining with frozen image encoders and large language models. In: International\\nConference on Machine Learning, pp. 19730–19742 (2023). PMLR\\n[62] Kim, W., Son, B., Kim, I.: Vilt: Vision-and-language transformer without convo-\\nlution or region supervision. In: International Conference on Machine Learning,\\npp. 5583–5594 (2021). PMLR\\n[63] Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before\\nfuse: Vision and language representation learning with momentum distillation.\\nAdvances in neural information processing systems 34, 9694–9705 (2021)\\n[64] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual mod-\\nels from natural language supervision. In: International Conference on Machine\\nLearning, pp. 8748–8763 (2021). PMLR\\n[65] Yu, T., Yao, Y., Zhang, H., He, T., Han, Y., Cui, G., Hu, J., Liu, Z., Zheng, H.-\\nT., Sun, M., et al.: Rlhf-v: Towards trustworthy mllms via behavior alignment\\nfrom fine-grained correctional human feedback. arXiv preprint arXiv:2312.00849\\n(2023)\\n[66] Bao, H., Dong, L., Piao, S., Wei, F.: Beit: Bert pre-training of image transform-\\ners. arXiv preprint arXiv:2106.08254 (2021)\\n27\\n[67] Reed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B., Lee, H.: Generative\\nadversarial text to image synthesis. In: International Conference on Machine\\nLearning, pp. 1060–1069 (2016). PMLR\\n[68] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M.,\\nSutskever, I.: Zero-shot text-to-image generation. In: International Conference\\non Machine Learning, pp. 8821–8831 (2021). Pmlr\\n[69] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and\\nediting with text-guided diffusion models. arXiv preprint arXiv:2112.10741\\n(2021)\\n[70] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\\nimage synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, pp. 10684–10695\\n(2022)\\n[71] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances\\nin neural information processing systems 33, 6840–6851 (2020)\\n[72] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion model in\\ngenerative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)\\n[73] Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings\\nof the IEEE/CVF International Conference on Computer Vision, pp. 4195–4205\\n(2023)\\n[74] Singh, A.: A survey of ai text-to-image and ai text-to-video generators. In: 2023\\n4th International Conference on Artificial Intelligence, Robotics and Control\\n(AIRC), pp. 32–36 (2023). IEEE\\n[75] Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang,\\nJ., Lee, J., Guo, Y., et al.: Improving image generation with better captions.\\nComputer Science. https://cdn. openai. com/papers/dall-e-3. pdf 2(3), 8 (2023)\\n[76] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr,\\nD., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video\\ngeneration models as world simulators (2024)\\n[77] Oppenlaender, J.: A taxonomy of prompt modifiers for text-to-image generation.\\nBehaviour & Information Technology, 1–14 (2023)\\n[78] Liu, V., Chilton, L.B.: Design guidelines for prompt engineering text-to-image\\ngenerative models. In: Proceedings of the 2022 CHI Conference on Human\\nFactors in Computing Systems, pp. 1–23 (2022)\\n28\\n[79] Hao, Y., Chi, Z., Dong, L., Wei, F.: Optimizing prompts for text-to-image\\ngeneration, pp. 1–17. Curran Associates Inc., Red Hook, NY, USA (2024)\\n[80] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto,\\nA., Fung, P.: Survey of hallucination in natural language generation. ACM\\nComputing Surveys 55(12), 1–38 (2023)\\n[81] Arefeen, M.A., Debnath, B., Chakradhar, S.: Leancontext: Cost-efficient\\ndomain-specific question answering using llms. Natural Language Processing\\nJournal 7, 100065 (2024)\\n[82] Aharoni, R., Goldberg, Y.: Unsupervised domain clusters in pretrained language\\nmodels. arXiv preprint arXiv:2004.02105 (2020)\\n[83] Li, H., Ai, Q., Chen, J., Dong, Q., Wu, Z., Liu, Y., Chen, C., Tian, Q.: Blade:\\nEnhancing black-box large language models with small domain-specific models.\\narXiv preprint arXiv:2403.18365 (2024)\\n[84] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., Wang,\\nH.: Retrieval-augmented generation for large language models: A survey. arXiv\\npreprint arXiv:2312.10997 (2023)\\n[85] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨uttler,\\nH., Lewis, M., Yih, W.-t., Rockt¨aschel, T., et al.: Retrieval-augmented gen-\\neration for knowledge-intensive nlp tasks. Advances in Neural Information\\nProcessing Systems 33, 9459–9474 (2020)\\n[86] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoy-\\nanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-training for\\nnatural language generation, translation, and comprehension. In: Proceedings of\\nthe 58th Annual Meeting of the Association for Computational Linguistics, pp.\\n7871–7880 (2020)\\n[87] Moratanch, N., Chitrakala, S.: A survey on extractive text summarization.\\nIn: 2017 International Conference on Computer, Communication and Signal\\nProcessing (ICCCSP), pp. 1–6 (2017). IEEE\\n[88] Lin, H., Ng, V.: Abstractive summarization: A survey of the state of the art.\\nIn: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, pp.\\n9815–9822 (2019)\\n[89] Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R.,\\nMcNamara, A., Mitra, B., Nguyen, T., Rosenberg, M., Song, X., Stoica, A.,\\nTiwary, S., Wang, T.: MS MARCO: A Human Generated MAchine Reading\\nCOmprehension Dataset (2018)\\n[90] Zhao, P., Zhang, H., Yu, Q., Wang, Z., Geng, Y., Fu, F., Yang, L., Zhang,\\n29\\nW., Cui, B.: Retrieval-augmented generation for ai-generated content: A survey.\\narXiv preprint arXiv:2402.19473 (2024)\\n[91] Asai, A., Min, S., Zhong, Z., Chen, D.: Retrieval-based language models and\\napplications. In: Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 6: Tutorial Abstracts), pp. 41–46 (2023)\\n[92] Guu, K., Lee, K., Tung, Z., Pasupat, P., Chang, M.: Retrieval augmented lan-\\nguage model pre-training. In: International Conference on Machine Learning,\\npp. 3929–3938 (2020). PMLR\\n[93] Ma, X., Gong, Y., He, P., Zhao, H., Duan, N.: Query rewriting for retrieval-\\naugmented large language models. arXiv preprint arXiv:2305.14283 (2023)\\n[94] Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J.,\\nChen, X., Lin, Y., et al.: A survey on large language model based autonomous\\nagents. Frontiers of Computer Science 18(6), 186345 (2024)\\n[95] Zhang, Z., Bo, X., Ma, C., Li, R., Chen, X., Dai, Q., Zhu, J., Dong, Z., Wen,\\nJ.-R.: A Survey on the Memory Mechanism of Large Language Model based\\nAgents (2024)\\n[96] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Optimizing dense retrieval\\nmodel training with hard negatives. In: Proceedings of the 44th International\\nACM SIGIR Conference on Research and Development in Information Retrieval,\\npp. 1503–1512 (2021)\\n[97] Robertson, S., Zaragoza, H., et al.: The probabilistic relevance framework: Bm25\\nand beyond. Foundations and Trends® in Information Retrieval 3(4), 333–389\\n(2009)\\n[98] Mao, S., Jiang, Y., Chen, B., Li, X., Wang, P., Wang, X., Xie, P., Huang, F.,\\nChen, H., Zhang, N.: Rafe: Ranking feedback improves query rewriting for rag.\\narXiv preprint arXiv:2405.14431 (2024)\\n[99] Chan, C.-M., Xu, C., Yuan, R., Luo, H., Xue, W., Guo, Y., Fu, J.: Rq-rag:\\nLearning to refine queries for retrieval augmented generation. arXiv preprint\\narXiv:2404.00610 (2024)\\n[100] Li, T., Zhang, G., Do, Q.D., Yue, X., Chen, W.: Long-context llms struggle with\\nlong in-context learning. arXiv preprint arXiv:2404.02060 (2024)\\n[101] Liu, N.F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., Liang,\\nP.: Lost in the middle: How language models use long contexts. Transactions of\\nthe Association for Computational Linguistics 12, 157–173 (2024)\\n[102] Faggioli, G., Dietz, L., Clarke, C.L., Demartini, G., Hagen, M., Hauff, C., Kando,\\n30\\nN., Kanoulas, E., Potthast, M., Stein, B., et al.: Perspectives on large lan-\\nguage models for relevance judgment. In: Proceedings of the 2023 ACM SIGIR\\nInternational Conference on Theory of Information Retrieval, pp. 39–50 (2023)\\n[103] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt,\\nand predict: A systematic survey of prompting methods in natural language\\nprocessing. ACM Comput. Surv. 55(9) (2023) https://doi.org/10.1145/3560815\\n[104] Wang, X., Yang, Q., Qiu, Y., Liang, J., He, Q., Gu, Z., Xiao, Y., Wang, W.:\\nKnowledgpt: Enhancing large language models with retrieval and storage access\\non knowledge bases. arXiv preprint arXiv:2308.11761 (2023)\\n[105] Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao,\\nC., Han, C., Fung, Y.R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang,\\nS., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai,\\nZ., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X.,\\nLi, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., Sun, M.: Tool Learning with\\nFoundation Models (2023)\\n[106] Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan,\\nJ., Neubig, G.: Active retrieval augmented generation. In: Bouamor, H., Pino,\\nJ., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing, pp. 7969–7992. Association for Computational\\nLinguistics, Singapore (2023). https://doi.org/10.18653/v1/2023.emnlp-main.\\n495 . https://aclanthology.org/2023.emnlp-main.495\\n[107] Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown,\\nK., Shoham, Y.: In-context retrieval-augmented language models. Transactions\\nof the Association for Computational Linguistics 11, 1316–1331 (2023)\\n[108] Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K.,\\nVan Den Driessche, G.B., Lespiau, J.-B., Damoc, B., Clark, A., et al.: Improv-\\ning language models by retrieving from trillions of tokens. In: International\\nConference on Machine Learning, pp. 2206–2240 (2022). PMLR\\n[109] Trivedi, H., Balasubramanian, N., Khot, T., Sabharwal, A.: Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions.\\nIn: Rogers, A., Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st\\nAnnual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), pp. 10014–10037. Association for Computational Linguis-\\ntics, Toronto, Canada (2023). https://doi.org/10.18653/v1/2023.acl-long.557 .\\nhttps://aclanthology.org/2023.acl-long.557\\n[110] Ni, S., Bi, K., Guo, J., Cheng, X.: When do llms need retrieval augmentation?\\nmitigating llms’ overconfidence helps retrieval augmentation. arXiv preprint\\narXiv:2402.11457 (2024)\\n31\\n[111] Su, W., Tang, Y., Ai, Q., Wu, Z., Liu, Y.: DRAGIN: Dynamic retrieval aug-\\nmented generation based on the real-time information needs of large language\\nmodels. In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Proceedings of the\\n62nd Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers), pp. 12991–13013. Association for Computational Linguis-\\ntics, Bangkok, Thailand (2024). https://doi.org/10.18653/v1/2024.acl-long.702\\n. https://aclanthology.org/2024.acl-long.702/\\n[112] Su, W., Wang, C., Ai, Q., Hu, Y., Wu, Z., Zhou, Y., Liu, Y.: Unsupervised\\nreal-time hallucination detection based on the internal states of large language\\nmodels. In: Ku, L.-W., Martins, A., Srikumar, V. (eds.) Findings of the Asso-\\nciation for Computational Linguistics: ACL 2024, pp. 14379–14391. Association\\nfor Computational Linguistics, Bangkok, Thailand (2024). https://doi.org/10.\\n18653/v1/2024.findings-acl.854 . https://aclanthology.org/2024.findings-acl.854/\\n[113] Liu, T., Zhang, Y., Brockett, C., Mao, Y., Sui, Z., Chen, W., Dolan, B.: A\\ntoken-level reference-free hallucination detection benchmark for free-form text\\ngeneration. arXiv preprint arXiv:2104.08704 (2021)\\n[114] Fadeeva, E., Rubashevskii, A., Shelmanov, A., Petrakov, S., Li, H., Mubarak, H.,\\nTsymbalov, E., Kuzmin, G., Panchenko, A., Baldwin, T., et al.: Fact-checking\\nthe output of large language models via token-level uncertainty quantification.\\narXiv preprint arXiv:2403.04696 (2024)\\n[115] Cronen-Townsend, S., Croft, W.B., et al.: Quantifying query ambiguity. In:\\nProceedings of HLT, vol. 2, pp. 94–98 (2002)\\n[116] ARENS, Y., CHEE, C.Y., HSU, C.-N., KNOBLOCK, C.A.: Retrieving and\\nintegrating data from multiple information sources. International Journal of\\nCooperative Information Systems 02(02), 127–158 (1993) https://doi.org/10.\\n1142/S0218215793000071 https://doi.org/10.1142/S0218215793000071\\n[117] Wang, J., Mo, F., Ma, W., Sun, P., Zhang, M., Nie, J.-Y.: A User-Centric\\nBenchmark for Evaluating Large Language Models (2024)\\n[118] Wang, J., Ma, W., Sun, P., Zhang, M., Nie, J.-Y.: Understanding User\\nExperience in Large Language Model Interactions (2024)\\n[119] Beitzel, S.M., Jensen, E.C., Chowdhury, A., Grossman, D., Frieder, O., Gohar-\\nian, N.: Fusion of effective retrieval strategies in the same information retrieval\\nsystem. Journal of the American Society for Information Science and Technology\\n55(10), 859–868 (2004)\\n[120] Wu, S., McClean, S.: Performance prediction of data fusion for information\\nretrieval. Information processing & management 42(4), 899–915 (2006)\\n32\\n[121] Cormack, G.V., Clarke, C.L., Buettcher, S.: Reciprocal rank fusion outper-\\nforms condorcet and individual rank learning methods. In: Proceedings of the\\n32nd International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, pp. 758–759 (2009)\\n[122] Lee, C.-J., Ai, Q., Croft, W.B., Sheldon, D.: An optimization framework for\\nmerging multiple result lists. In: Proceedings of the 24th ACM International on\\nConference on Information and Knowledge Management, pp. 303–312 (2015)\\n[123] Liu, T.-Y., et al.: Learning to rank for information retrieval. Foundations and\\nTrends® in Information Retrieval 3(3), 225–331 (2009)\\n[124] Zhan, J., Mao, J., Liu, Y., Zhang, M., Ma, S.: Learning to retrieve: How to train a\\ndense retrieval model effectively and efficiently. arXiv preprint arXiv:2010.10469\\n(2020)\\n[125] Arora, D., Kini, A., Chowdhury, S.R., Natarajan, N., Sinha, G., Sharma,\\nA.: Gar-meets-rag paradigm for zero-shot information retrieval. arXiv preprint\\narXiv:2310.20158 (2023)\\n[126] Zhang, T., Patil, S.G., Jain, N., Shen, S., Zaharia, M., Stoica, I., Gonzalez, J.E.:\\nRAFT: Adapting Language Model to Domain Specific RAG (2024)\\n[127] Xu, Z., Tran, A., Yang, T., Ai, Q.: Reinforcement learning to rank with coarse-\\ngrained labels. arXiv preprint arXiv:2208.07563 (2022)\\n[128] Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer,\\nL., Yih, W.-t.: Replug: Retrieval-augmented black-box language models. arXiv\\npreprint arXiv:2301.12652 (2023)\\n[129] Ai, Q., Bai, T., Cao, Z., Chang, Y., Chen, J., Chen, Z., Cheng, Z., Dong, S.,\\nDou, Z., Feng, F., et al.: Information retrieval meets large language models: a\\nstrategic report from chinese ir community. AI Open 4, 80–90 (2023)\\n[130] Bota, H., Zhou, K., Jose, J.M., Lalmas, M.: Composite retrieval of heterogeneous\\nweb search. In: Proceedings of the 23rd International Conference on World Wide\\nWeb, pp. 119–130 (2014)\\n[131] Amer-Yahia, S., Bonchi, F., Castillo, C., Feuerstein, E., Mendez-Diaz, I.,\\nZabala, P.: Composite retrieval of diverse and complementary bundles. IEEE\\nTransactions on Knowledge and Data Engineering 26(11), 2662–2675 (2014)\\n[132] Kolomiyets, O., Moens, M.-F.: A survey on question answering technology from\\nan information retrieval perspective. Information Sciences 181(24), 5412–5434\\n(2011)\\n[133] Metzler, D., Tay, Y., Bahri, D., Najork, M.: Rethinking search: making domain\\n33\\nexperts out of dilettantes. SIGIR Forum 55(1) (2021) https://doi.org/10.1145/\\n3476415.3476428\\n[134] Zhuang, S., Ren, H., Shou, L., Pei, J., Gong, M., Zuccon, G., Jiang, D.: Bridging\\nthe Gap Between Indexing and Retrieval for Differentiable Search Index with\\nQuery Generation (2023). https://arxiv.org/abs/2206.10128\\n[135] Tay, Y., Tran, V., Dehghani, M., Ni, J., Bahri, D., Mehta, H., Qin, Z., Hui, K.,\\nZhao, Z., Gupta, J., et al.: Transformer memory as a differentiable search index.\\nAdvances in Neural Information Processing Systems 35, 21831–21843 (2022)\\n[136] Tang, Y., Zhang, R., Guo, J., Chen, J., Zhu, Z., Wang, S., Yin, D., Cheng, X.:\\nSemantic-enhanced differentiable search index inspired by learning strategies.\\nIn: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\\nand Data Mining, pp. 4904–4913 (2023)\\n[137] Sun, W., Yan, L., Chen, Z., Wang, S., Zhu, H., Ren, P., Chen, Z., Yin, D., Rijke,\\nM., Ren, Z.: Learning to tokenize for generative retrieval. In: Proceedings of the\\n37th International Conference on Neural Information Processing Systems. NIPS\\n’23, pp. 1–17. Curran Associates Inc., Red Hook, NY, USA (2024)\\n[138] Nguyen, T., Yates, A.: Generative retrieval as dense retrieval. arXiv preprint\\narXiv:2306.11397 (2023)\\n[139] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Learning discrete repre-\\nsentations via constrained clustering for effective and efficient dense retrieval. In:\\nProceedings of the Fifteenth ACM International Conference on Web Search and\\nData Mining. WSDM ’22, pp. 1328–1336. Association for Computing Machin-\\nery, New York, NY, USA (2022). https://doi.org/10.1145/3488560.3498443 .\\nhttps://doi.org/10.1145/3488560.3498443\\n[140] Zeng, H., Luo, C., Jin, B., Sarwar, S.M., Wei, T., Zamani, H.: Scalable and\\neffective generative information retrieval. In: Proceedings of the ACM on Web\\nConference 2024. WWW ’24, pp. 1441–1452. Association for Computing Machin-\\nery, New York, NY, USA (2024). https://doi.org/10.1145/3589334.3645477 .\\nhttps://doi.org/10.1145/3589334.3645477\\n[141] Zeng, H., Luo, C., Zamani, H.: Planning Ahead in Generative Retrieval: Guiding\\nAutoregressive Generation through Simultaneous Decoding (2024)\\n[142] Wu, S., Wei, W., Zhang, M., Chen, Z., Ma, J., Ren, Z., Rijke, M., Ren,\\nP.: Generative retrieval as multi-vector dense retrieval. In: Proceedings of the\\n47th International ACM SIGIR Conference on Research and Development in\\nInformation Retrieval, pp. 1828–1838 (2024)\\n[143] Zhan, J., Mao, J., Liu, Y., Guo, J., Zhang, M., Ma, S.: Jointly optimizing query\\n34\\nencoder and product quantization to improve retrieval performance. In: Proceed-\\nings of the 30th ACM International Conference on Information & Knowledge\\nManagement. CIKM ’21, pp. 2487–2496. Association for Computing Machin-\\nery, New York, NY, USA (2021). https://doi.org/10.1145/3459637.3482358 .\\nhttps://doi.org/10.1145/3459637.3482358\\n[144] Sachidananda, V., Kessler, J.S., Lai, Y.-A.: Efficient domain adaptation of\\nlanguage models via adaptive tokenization. arXiv preprint arXiv:2109.07460\\n(2021)\\n[145] Huang, Q., Tao, M., Zhang, C., An, Z., Jiang, C., Chen, Z., Wu, Z., Feng, Y.:\\nLawyer llama technical report. arXiv preprint arXiv:2305.15062 (2023)\\n[146] Cui, J., Li, Z., Yan, Y., Chen, B., Yuan, L.: Chatlaw: Open-source legal\\nlarge language model with integrated external knowledge bases. arXiv preprint\\narXiv:2306.16092 (2023)\\n[147] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,\\nP., Rosenberg, D., Mann, G.: Bloomberggpt: A large language model for finance.\\narXiv preprint arXiv:2303.17564 (2023)\\n[148] Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., Wei, F.: Knowledge neurons in\\npretrained transformers. arXiv preprint arXiv:2104.08696 (2021)\\n[149] Meng, K., Bau, D., Andonian, A., Belinkov, Y.: Locating and editing factual\\nassociations in gpt. Advances in Neural Information Processing Systems 35,\\n17359–17372 (2022)\\n[150] Liu, J., Yu, P., Zhang, Y., Li, S., Zhang, Z., Ji, H.: Evedit: Event-based knowl-\\nedge editing with deductive editing boundaries. arXiv preprint arXiv:2402.11324\\n(2024)\\n[151] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\\narXiv:2106.09685 (2021)\\n[152] Li, H., Ai, Q., Chen, J., Dong, Q., Wu, Y., Liu, Y., Chen, C., Tian, Q.: Sailer:\\nstructure-aware pre-trained language model for legal case retrieval. In: Pro-\\nceedings of the 46th International ACM SIGIR Conference on Research and\\nDevelopment in Information Retrieval, pp. 1035–1044 (2023)\\n[153] Yao, F., Li, C., Nekipelov, D., Wang, H., Xu, H.: Human vs. Generative AI in\\nContent Creation Competition: Symbiosis or Conflict? (2024)\\n35\\n'), Document(metadata={'Published': '2026-01-23', 'Title': 'Competing Visions of Ethical AI: A Case Study of OpenAI', 'Authors': 'Melissa Wilfley, Mengting Ai, Madelyn Rose Sanfilippo', 'Summary': \"Introduction. AI Ethics is framed distinctly across actors and stakeholder groups. We report results from a case study of OpenAI analysing ethical AI discourse. Method. Research addressed: How has OpenAI's public discourse leveraged 'ethics', 'safety', 'alignment' and adjacent related concepts over time, and what does discourse signal about framing in practice? A structured corpus, differentiating between communication for a general audience and communication with an academic audience, was assembled from public documentation. Analysis. Qualitative content analysis of ethical themes combined inductively derived and deductively applied codes. Quantitative analysis leveraged computational content analysis methods via NLP to model topics and quantify changes in rhetoric over time. Visualizations report aggregate results. For reproducible results, we have released our code at https://github.com/famous-blue-raincoat/AI_Ethics_Discourse. Results. Results indicate that safety and risk discourse dominate OpenAI's public communication and documentation, without applying academic and advocacy ethics frameworks or vocabularies. Conclusions. Implications for governance are presented, along with discussion of ethics-washing practices in industry.\"}, page_content=' \\nInformation Research - Vol. ?? No. ? (20??) \\nInformation Research, Vol. ?? No. ? (20??) \\n1 \\nCompeting Visions of Ethical AI: A Case Study of OpenAI \\nMelissa Wilfley*, Mengting Ai*, Madelyn Rose Sanfilippo \\nAbstract \\nIntroduction. AI Ethics is framed distinctly across actors and stakeholder groups. \\nWe report results from a case study of OpenAI analysing ethical AI discourse. \\nMethod. Research addressed: How has OpenAI’s public discourse leveraged ‘ethics’, \\n‘safety’, ‘alignment’ and adjacent related concepts over time, and what does \\ndiscourse signal about framing in practice? A structured corpus, differentiating \\nbetween communication for a general audience and communication with an \\nacademic audience, was assembled from public documentation. \\nAnalysis. Qualitative content analysis of ethical themes combined inductively \\nderived \\nand \\ndeductively \\napplied \\ncodes. \\nQuantitative \\nanalysis \\nleveraged \\ncomputational content analysis methods via NLP to model topics and quantify \\nchanges in rhetoric over time. Visualizations report aggregate results. For \\nreproducible results, we have released our code at https://github.com/famous-\\nblue-raincoat/AI_Ethics_Discourse. \\nResults. Results indicate that safety and risk discourse dominate OpenAI’s public \\ncommunication and documentation, without applying academic and advocacy \\nethics frameworks or vocabularies. \\nConclusions. Implications for governance are presented, along with discussion of \\nethics-washing practices in industry. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n* Equal Contribution. \\n \\nInformation Research, Vol. ?? No. ? (20??) \\n2 \\nIntroduction \\nArtificial intelligence (AI) is advancing at a pace faster than the vocabularies and institutions meant \\nto govern it (Cath, 2018; Lazar & Nelson, 2023). General-purpose systems such as large language \\nmodels (LLMs), image generators and multimodal agents now circulate across work, politics and \\neveryday life, producing sociotechnical dynamics that high-level ethical principles struggle to \\ncapture. Widely cited frameworks organized around beneficence, non-maleficence, justice, \\nautonomy and transparency (Floridi & Cowls, 2019; Jobin et al., 2019) offer broad guidance, yet \\nscholars argue these commitments frequently collapse when confronted with infrastructures, \\nincentives and usage paradigms that outpace institutional capacity (Mittelstadt, 2019; Morley et al., \\n2021). \\nWe approach ‘AI ethics’ not as a checklist of principles but as a discursive and generative field: one \\nthat sets expectations, legitimizes actors, shapes policy debates, and structures organizational \\npractice and labour roles. Given that governance infrastructures and technical standards are still \\nunder construction, language does constitutive work (Hajer, 1995; Fairclough, 1992). The words a \\nleading AI developer chooses, or avoids, help define what counts as responsible practice, whose \\nexpertise matters, and which risks are foregrounded or sidelined (Stamboliev & Christiaens, 2025). \\nIf, for example, ethics is invoked only rhetorically, it can function more as reputational cover than \\nas a binding constraint (Bietti, 2020; Green, 2021). Language choices reallocate epistemic authority \\nin a field where the stakes are high, and the rules are still being written. \\nPrior work maps ethical principles (Jobin et al., 2019), critiques corporate rhetoric (Green, 2021), \\nand assesses governance frameworks (Papagiannidis et al., 2025; Morley et al., 2021). Few studies, \\nhowever, have traced over time how a leading AI developer publicly deploys, or omits, explicit \\nethics vocabulary relative to adjacent terms such as safety and alignment. We address the following \\nresearch question: How has OpenAI’s public discourse leveraged ethics, safety, alignment and \\nadjacent concepts over time, and what does discourse signal about ethical framing in practice?  \\nBackground \\nPrinciple-based AI ethics \\nAI ethics scholarship often centres on principle-based frameworks that translate ethical theory \\ninto high-level commitments for sociotechnical AI systems. Widely cited approaches converge \\naround beneficence and non-maleficence, autonomy, justice and explicability/transparency \\n(Floridi & Cowls, 2019; Jobin et al., 2019). These principles emphasize rights, fairness, and \\naccountability as values to guide AI design and deployment. At the same time, scholars increasingly \\ncaution that principles can remain abstract or symbolic, without mechanisms for enforcement \\n(Mittelstadt, 2019). More recent work highlights the persistent gap between principles and \\npractice, showing that as AI systems scale, organizations struggle to embed such normative \\ncommitments into day-to-day development, evaluation and release processes (Ryan et al., 2021; \\nStahl, 2022).  \\nPrinciples-to-Practice \\nAI developers have recently rolled out ‘responsible AI’ or ‘AI safety’ programs, that attempt to \\noperationalize ethics through risk management processes, evaluation benchmarks, incident \\nreporting, and model governance. This translation has clear benefits in offering technical fixes (e.g. \\nbias audits and fairness benchmarks: Raji et al., 2020; Barocas et al., 2023; model cards: Mitchell et \\nal., 2019; datasheets: Gebru et al., 2021), creating internal governance processes (e.g. incident logs, \\nrisk tiers and review boards: Raji et al., 2020; Morley et al., 2021; Metcalf et al., 2021), and \\nestablishing common standards that allow developers to coordinate around shared goals (IEEE, \\n2019; Ryan et al., 2021; Morley et al., 2021). However, there are trade-offs associated. Toolkits tend \\nto be either too general to be practically useful or too rigid to adapt to specific sociotechnical \\nInformation Research, Vol. ?? No. ? (20??) \\n3 \\ncontexts, leaving a persistent principle-to-practice gap (Morley et al., 2021; Metcalf et al., 2021; \\nHerzog & Blank, 2024). Scholars also warn that corporate calls for ‘responsibility’ can function as \\ngovernance-lite, where vague signals, like publishing an ethics statement, can be substituted for \\nsubstantive accountability (Mittelstadt, 2019; Metcalf et al., 2021; Green, 2021; Stahl et al., 2022).  \\nEthics-washing and Reputational Ethics \\nAI ethics-washing (and related concepts) conceptually labels cases where ‘ethics’ becomes a \\nreputational asset rather than a binding constraint (Bietti, 2020; van Maanen, 2022; Seele & Schultz, \\n2022). Ethics then collapses into a communications layer of values pages and principles documents, \\nwhile real decision-making is structured by product roadmaps, release cycles, partnerships and \\nlabour priorities. Companies tend to selectively use procedural vocabularies such as ‘safety’, ‘trust’, \\nand ‘responsibility’, while sidelining explicitly political or justice-oriented language that can appear \\nmore disruptive within institutional settings (Stamboliev & Christiaens, 2025; Greene et al., 2019; \\nGreen, 2021; Heilinger, 2022). Ethnographic and interview-based studies further reveal how ‘ethics \\nowners’ institutionalize responsibility as a matter of process management rather than moral \\nconstraint (Metcalf et al., 2019), how AI practitioners feel responsibility displaced by powerful \\ninstitutional actors (Orr & Davis, 2020), and how developers gain ethical expertise even as ethics \\ndebates are displaced outside their communities (Griffin et al., 2025). Hao (2025) argues that \\nunenforceable ethical statements normalize secrecy and consolidation while masking departures \\nfrom earlier commitments to openness and collaboration. \\nDiscourse \\nFrom sociotechnical and discourse perspectives, organizational language does constitutive work: \\nit legitimizes actors, defines problems and solutions, and sets boundaries on what ‘responsible \\ndevelopment’ can look like (Hajer, 1995; Fairclough, 1992). Technical processes and documents are \\nnot neutral, they script norms and values into industry practices (e.g., Shilton, 2018). Scholars have \\nshown how corporate vocabularies shape ethical debates by institutionalizing responsibility as \\nmanagerial process instead of a moral constraint (Metcalf et al., 2019), framing value statements to \\nenable some conversations while foreclosing on justice-oriented alternatives (Greene et al., 2019) \\nand narrowing ethical discourse toward techno-solutionism (Heilinger, 2022).  \\nFor example, Hao (2025) illustrates this discursive power at OpenAI. Altman framed companies as \\n‘religions’ (p. 9), Sutskever redefined ‘openness’ as an openness in spirit, and Brockman cast AI as \\na neutral actor serving humanity (p. 42). Executives further positioned ‘safety’ existentially, \\nforegrounding catastrophic risk scenarios (Our Updated Preparedness Framework, 2025) while \\ndeflecting attention from nearer-term AI ethical concerns like labour displacement (U.S. Senate \\nCommittee on the Judiciary, Subcommittee on Privacy, Technology, and the Law., 2023; Hao, 2025). \\nSuch discourse shapes norms, privileging and delegitimizing certain risks and responsibilities. \\nMethod \\nWe employed a mixed-methods approach to examine how OpenAI constructs and deploys ethics \\ndiscourse in its public communications between December 2015 and July 2025. Our analysis \\ncombined computational text analysis to capture large-scale quantitative patterns with a \\nqualitative audit to interpret the contextual nuances of ethics related terminology. This design \\ntraces both the frequency and the faming of ‘ethics’, ‘safety’ and adjacent concepts across OpenAI’s \\nweb articles and publications. \\nCase Selection \\nFour ex-ante criteria were identified for a suitable case for longitudinal analysis of AI ethics \\ndiscourse: 1) Frontier AI development: an organization developing and deploying general-\\npurpose/foundational models at the technological frontier; 2) Ethics discourse participation: the \\ncompany has made public commitments and authored or co-authored recurring use of terms such \\nas ‘ethics’, ‘safety’, ‘alignment’, and ‘responsible AI’ for at least five years; 3) Temporal traceability: \\nInformation Research, Vol. ?? No. ? (20??) \\n4 \\na dated, publicly accessible archive of news, research posts and linked publications that enables \\ntime-series analysis; 4) Scale and Impact: models surpassing the general-purpose AI threshold \\nguidelines of 1023 FLOPs (approx. 1B parameters) as defined in the EU Artificial Intelligence Act \\n(Regulation (EU) 2024/1689). \\nOpenAI uniquely satisfies all four criteria. Selecting a single, information-rich case allows us to \\ncombine large-scale computational measures with close qualitative auditing while holding \\norganizational context constant. \\nCorpus Construction and Preprocessing \\nTo ensure that the analysis reflected discourse generated by OpenAI itself, we restricted our \\ndataset to OpenAI-authored and co-authored materials published through its primary public \\ncommunications channel (OpenAI.com) and its main research dissemination venue (arXiv), where \\nOpenAI routinely releases preprints of its formal publications. Our corpus was collected from two \\nsources to enable both quantitative and qualitative analysis:  \\nOpenAI Website Articles (n = 424): We systematically collected all publicly available web articles \\nfrom OpenAI’s News (OpenAI research, 2025) and Research (OpenAI news, 2025) sections.  \\nPublications (n = 30): To capture OpenAI’s research register, we included OpenAI-authored and \\nco-authored publications that the organisation either hosted directly or linked through its website. \\nBecause most publications surfaced on OpenAI.com also appeared on arXiv, we supplemented the \\nsite-hosted corpus by systematically identifying additional OpenAI-authored or co-authored \\npublications. This set contains: \\na) arXiv preprints (n = 25): From 137 initially identified, 26 addressed ethics; one was \\nexcluded due to inaccessibility via OpenAI.com, yielding 25. \\nb) Site-hosted items (n = 5): The website surfaced 43 publications via primary call-to-\\naction (CTA) links—29 PDFs hosted on OpenAI’s CDN and 14 HTML reports. Of these, only \\n3 CDN PDFs and 2 HTML reports contained a variation of the term ethic (e.g., ethic, ethics, \\nethical) and were not duplicates,  \\nThe total corpus includes 454 documents: 424 web articles and 30 publications (25 arXiv preprints \\nand 5 site-hosted items). While we analyse publications as a single category, we note their distinct \\nprovenance and, where relevant, report findings separately for arXiv and site-hosted outputs. \\nAll texts were processed through a standardized pipeline: lowercasing, removal of punctuation, \\ndigits, and HTML artifacts, and lemmatization. Stopwords were removed using a standard English \\nlist augmented with corpus-specific terms (e.g., openai, gpt, window). This ensured conceptual \\nconsistency across corpora and minimized noise in downstream analysis. For web articles, \\nparatextual material was excluded, such as footnotes, references, contributors and \\nacknowledgements. For publications, only the title, subtitle, abstract and body text were included. \\nThis ensured the analysis focused on substantive discursive content instead of bibliographic or \\nparatextual methods of ethics. The dataset reflects curated, English-language materials OpenAI \\nelected to publish, representing a snapshot versus a complete archival record of all prior discourse. \\nQuantitative Analysis \\nOur computational framework consisted of two complementary methods: (1) a targeted keyword-\\nconcept frequency analysis to track pre-defined concepts, and (2) an unsupervised topic modeling \\napproach to discover emergent thematic structures. \\nThe first pillar of our quantitative analysis aimed to quantify the prevalence and chronological \\nshifts of known, pre-defined ethics and safety concepts. \\nInformation Research, Vol. ?? No. ? (20??) \\n5 \\n1) Concept Library Construction: We developed a comprehensive keyword library \\norganized by core concepts. We manually grouped related terms and their variations (e.g., \\ngrouping safe, safety, and safely under a unified SAFETY concept) to ensure that the analysis \\ntracked high-level ideas rather than just individual words. The final library consisted of 75 \\ncore concepts. \\n \\n2) Frequency Tracking & Visualization: After standardized preprocessing, we calculated \\nconcepts’ annual frequencies. The results were visualized using heatmaps, which allowed \\nfor a clear comparison of concept prominence over time and revealed distinct patterns \\nbetween different document types, such as official PDF reports and more informal web \\narticles. This method provides a robust, quantitative baseline of how OpenAI discusses \\nspecific, pre-identified topics. \\nThe second pillar moved beyond pre-defined keywords to discover the latent thematic structures \\nand conceptual groupings. This bottom-up approach allows for the discovery of nuanced or \\nunexpected themes. \\n1) N-gram Vector Representation: We extracted significant bigrams and trigrams from the \\npreprocessed corpus to serve as proxies for core concepts. Each of these n-grams was \\nthen \\nencoded \\ninto \\na \\nhigh-dimensional \\nvector \\nusing \\nthe \\nall-MiniLM-L6-v2 \\nSentenceTransformer model (Reimers & Gurevych, 2019), which captures their semantic \\nmeaning in context. \\n \\n2) Semantic Clustering: We applied the HDBSCAN (McInnes et al., 2017) density-based \\nclustering algorithm to these n-gram vectors. Unlike methods that require a pre-specified \\nnumber of clusters (e.g. K-Means: Lloyd, 1982), HDBSCAN automatically determines the \\nnumber of emergent clusters based on the density of the semantic space, providing \\ncontextual grouping of concepts. \\n \\n3) Manual Thematic Labeling & Visualization: The resulting clusters, each containing a set \\nof semantically related n-grams, were manually reviewed and assigned thematic labels \\n(e.g., ‘Reinforcement Learning Policy,’ ‘Public Data & Security Controls’). We generated \\nvisualizations to map thematic clusters and illustrate relationships, providing a discourse \\nconcept map over time. \\nQualitative Analysis \\nTo further validate and contextualize computational findings, we conducted a systematic manual \\naudit of OpenAI web articles that contained ‘ethic-’, were tagged ‘Ethics & Safety’ in Research and \\n‘Safety’ in News, and their primary call-to-action (CTA) linked publications. We also reviewed all \\nOpenAI authored or co-authored arXiv preprints that contained the term ‘ethic-’. \\nEach item was logged in a structured matrix that captured both metadata (document type, year, \\nauthorship, URL, tags, and source location on OpenAI.com, arXiv, or CDN) and ethics-related usage \\n(e.g., whether ‘ethic’ appeared in the title, abstract, executive summary, section headers, diagrams, \\nor body text). For posts with CTA links, the linked publication was downloaded and recorded in the \\nmatrix. \\nWeb articles and publications were coded for explicit use of the term ethics as well as adjacent \\nmoral language (moral, values, norms). Coding tracked both frequency and framing, noting term \\nlocation in the web article or publication.  \\nInformation Research, Vol. ?? No. ? (20??) \\n6 \\nThe manually identified subset was cross-checked against the computational corpus to confirm \\ncoverage and resolve any discrepancies. This ensured that all items meeting the inclusion criteria \\nwere represented in both quantitative and qualitative analyses. \\nAdditionally, we conducted exploratory check to ensure adequate coverage. First, we performed a \\nlandscape audit of other OpenAI.com properties using a targeted search query (‘site:openai.com \\nethics’) to capture results across the site and related subdomains. Second, we used the Internet \\nArchive’s Wayback Machine to spot-check historical snapshots of OpenAI’s News and Research \\npages, focusing on changes to topic categories and tagging practices over time. ArXiv preprint \\ntitles were cross-referenced on OpenAI.com to identify official links. These supplementary steps \\nwere not exhaustive but provided additional confidence in the completeness of our dataset and \\nhelped identify edge cases where tagging practices had shifted, or where ethics discourse was \\napplied. \\nAmbiguous or missing cases were reviewed collaboratively by at least two researchers before \\ninclusion or exclusion. This collaborative process aimed to minimize bias and ensure consistency. \\nResults \\nUse of Ethics in OpenAI.com Web Articles \\nAcross the 424 web articles in our OpenAI.com corpus, explicit references to ethics were rare. Only \\n16 articles (3.8%) contained a variation of the term (ethic(s), ethical, unethical, ethicist(s)), making it \\none of the least frequently used vocabularies in the AI Ethics discursive dataset. By contrast, other \\nethics-adjacent terms appear far more frequently but undergo sharp temporal expansions. \\nSafe/safety (lemmatized) peaks at nearly 700 mentions, over two orders of magnitude higher than \\nethics. Risk (386 in 2024) and responsible (~50 mentions annually after 2023) also surge, while \\nalignment and governance emerge only after 2019, reflecting OpenAI’s shift towards technical and \\ninstitutional framings.  \\nFigure 1. Annual counts for focal keywords on OpenAI.com web articles (tabular view). The table highlights \\nthe 2023-2024 peak for safety and risk, alongside the persistently low frequency of ethics. \\nTemporal patterns (Figure 1 & 2) are revealing. Mentions of ethics per year peak at just seven in \\n2024. By contrast, safe/safety reaches 687 mentions, and risk is mentioned 386 times in 2024. Other \\nterms show delayed but notable growth—alignment is negligible until 2019 and then rises to 57 \\nmentions by 2022, clustering around n-gram phrases such as alignment problems, alignment \\nresearch, and deliberative alignment. Governance remains scarce until 2023-2024, when it registers \\na visible uptick. The aspirational phrase ‘benefitting humanity’ is modest (≤ 18), while responsible \\nfollows a similar trajectory with a low frequency until 2023, then spiking to ~50 mentions and \\nremaining elevated through 2025. \\nInformation Research, Vol. ?? No. ? (20??) \\n7 \\n \\nFigure 2. AI ethics keyword frequency heatmap of OpenAI.com web articles (2015–2025). The heatmap \\nvisualizes temporal trends, showing how ethics remains rare in discourse compared to growing prominence \\nof concepts such as safety, risk, alignment, responsibility, trustworthy and governance. \\nOur qualitative audit shows that even within the 16 web articles that reference ethics, usage is \\nminimal and largely superficial. 11 web articles mention the term only once, and the remaining 5 \\nno more than three times. The highest concentration occurs in the 2024 announcement “OpenAI \\nappoints Scott Schools as Chief Compliance Officer”, where the ethics is used solely to describe \\nSchools’ prior role as ‘Chief Ethics and Compliance Officer at Uber Technologies’ (OpenAI Appoints \\nScott Schools as Chief Compliance Officer, 2024). Nearly all other references to ethics are in passing, \\noften in stock phrases such as ‘ethical standards’ (Pioneering an AI Clinical Copilot with Penda \\nHealth, 2025) or ‘ethical innovation’ (OpenAI’s Commitment to Child Safety, 2024), rather than a \\nsustained analytic frame. \\nQualitative analysis of OpenAI’s web articles confirm this uneven distribution in tagging practices. \\nAs of July 2025, 52 Research articles were tagged ‘Ethics & Safety’ (OpenAI Research, 2025), yet only \\n7 of these used ethics in the body text. In the News section, 38 articles were tagged ‘Safety’ (OpenAI \\nNews, 2025), but only 3 used included ethics. Safety-tags are not evenly distributed across years: \\nthey appear in 2016, resurface in 2020, and then reoccur more consistently from 2022-2025. The \\nnumber rises sharply in 2023 (13 articles, compared to just 2 in 2022), followed by 10 in 2024 and 11 \\nby mid-2025. By contrast, ‘Ethics & Safety’ tags appear steadily from 2016-2025, but with little \\nsubstantive use of ethics in the body.  \\nImportantly, tagging conventions themselves have shifted over time. For instance, the article \\n“Planning for AGI and beyond” (2023) was initially tagged ‘Safety & Alignment’ (2023; Wayback \\nMachine) but now appears only under ‘Safety’. This suggests that tagging functions less as a stable \\nclassification system and more as a dynamic metadata practice used as a form of discursive \\nsignalling, reflecting changes in how OpenAI curates and publicly frames AI development. \\nThese trends point to a broader discursive narrowing in OpenAI’s public communications. The \\nwebsite overwhelmingly foregrounds safety and risk, with annual frequencies in the hundreds \\ncompared to single-digit mentions of ethics. Consistent with our qualitative audit, when ethics does \\nInformation Research, Vol. ?? No. ? (20??) \\n8 \\nappear, it is typically peripherally, used as a single reference in a header or brief line in the article \\nbody such as ‘uphold the highest ethical standards’ (Pioneering an AI Clinical Copilot with Penda \\nHealth, 2025). During this time frame, ethics functions more rhetorically than substantively, its \\nmarginal presence standing in stark contrast to the hundreds of mentions of safety and risk that \\ndominate OpenAI’s public voice. \\nOpenAI-Authored and Co-Authored Publications \\nExplicit references to ethics were far more prevalent across OpenAI-authored and co-authored \\npublications, though still far from frequent. Out of 180 publications reviewed, 31 contained a \\nvariation of the term ethic (17.2%), more than four times higher than the 3.8% rate observed in \\nOpenAI’s web articles. The bulk of these came from arXiv (26/137, 19.0%), with a smaller share from \\nsite hosted materials (5/43, 11.6%). \\nOf the 31 texts that used ethics language, all but one were accessible via OpenAI’s website. The final \\npublications corpus therefore consists of 30 items. Within this set, references were typically \\nminimal: 80% (24/30) mention the term three times or fewer, with only a handful exceeding five \\nmentions. The most notable frequencies occur in “The Malicious Use of Artificial Intelligence” \\n(2018, 9 mentions), “AI Safety Needs Social Scientists” (2019, 11 mentions), and “Toward Trustworthy \\nAI Development” (2020, 12 mentions). Our qualitative audit shows that when ethics does appear, it \\nusually falls into three recurring contexts: (1) external references, appendices, footnotes or \\ndatasets; (2) policy and guideline copy; or (3) appeals to societal norms and values. Like OpenAI’s \\nweb articles, this is often located in headers, call-out boxes, diagrams or peripheral framing devices \\nrather than sustained analytic engagement (see Table 1). Related terms such as morals, values and \\nnorms surfaced only sporadically, such as in stock phrases (‘align with human values’; Mu et al., \\n2024), or references to social dynamics (‘diffusion of bad norms’; OpenAI et al., 2024). Adjacent \\nterms like bias, harm or discrimination rarely co-occur with ethics and were framed in technical \\nor reputational registers rather than moral registers. \\nUse context \\nExample \\nExternal references, \\nappendices, footnotes, or \\ndatasets \\nCiting an AI ethics research dataset named ‘ETHICS’ (Burns et al., 2023) \\nor referencing ethical guidelines in a collab workshop paper (Shoker & \\nReddie, 2023). \\nPolicy/guideline content \\nTechnical documents (like system cards or model specs) mention ‘ethical’ \\nor ‘unethical’ behavior in example scenarios of disallowed content (Sora \\nsystem card, 2024), to label misuse (e.g. forging docs as ‘illegal or unethical’ \\nin a model reasoning example) (Guan et al., 2025). \\nInvoking societal \\nnorms/values \\nReferences to ‘ethical standards’ (OpenAI’s Approach to AI and National \\nSecurity, 2024) or rhetorical headers such as ‘Continued safety, policy, \\nand ethical alignment’ (Operator System Card, 2025). \\nTable 1. Where ethics language appears within publications. \\nAuthorship patterns provide further context. Of 30 publications, 13 were co-authored (2018-2023, \\n2025) by OpenAI. Two of the three most-ethics dense texts were co-authored: “Toward \\nTrustworthy AI Development” (2020; 12 mentions) and “The Malicious Use of AI” (2018; 9 mentions). \\nThis suggests that collaborations with academic and external partners create stronger incentives \\nto explicate ethics. \\nAudience also matters. Most publications are available via arXiv, situating them in an academic \\nregister where at least some reference to ethics is expected. By contrast, OpenAI’s website articles, \\nInformation Research, Vol. ?? No. ? (20??) \\n9 \\ntargeted to broader audiences, largely displace ethics discourse in favour of safety. These patterns \\nindicate that ethics is a discursive register activated most visibly in academic and collaborative \\ncontexts.  \\nCorpus-wide structure of OpenAI \\nConsidering the entire corpus (424 web articles and 180 machine-readable publications), there are \\nsharp contrasts in how OpenAI structures discourse temporally and structurally across public-\\nfacing and academic registers. \\nFigure 3. Keyword frequency heatmaps across OpenAI corpora (2015-2025). Left: web articles. Right: \\npublications (PDFs and HTML). Safety and risk dominate both corpora, while ethics remains marginal \\nthroughout. \\nTemporal patterns, shown in Figure 3, highlight nine focal keyword frequencies across web articles \\n(left) and publications (right). In web articles, safety and risk dominate, rising sharply after 2022 \\nand peaking in 2024, while ethics remains extremely rare across the full decade. Terms such as \\nresponsible and alignment also surge after 2021, tracking OpenAI’s pivot toward technical and \\ngovernance-adjacent framings.  \\nIn publications, policy registers an early peak (2017-2019) before declining in relative prominence. \\nSafety and risk remain consistently present, with visible spikes in 2019 and 2023. Publications \\nreveal shifting foci: policy anchors discourse in the late 2010s, safety and risk become prominent \\nby the early 2020s, and alignment emerges more recently as a technical focal point. \\n \\nFigure 4. PCA clustering of OpenAI corpora. Left: web articles. Right: publications (PDFs and HTMLs.) \\nPCA clustering reveals a structural divergence between OpenAI’s web articles and its publications \\n(see Figure 4). The web articles illustrate integrated discussion where many topics overlap, creating \\na kind of ‘melting pot’. In comparison, OpenAI-authored and co-authored publications show \\nInformation Research, Vol. ?? No. ? (20??) \\n10 \\ngreater thematic specialization, with some topics being very distinct while others are still \\nintertwined.  \\nIn the web articles, topics such as ‘Safety Engineering & Reliability’ (grey), ‘Governance & \\nResponsible Practices’ (green), ‘Security & Adversarial Robustness’ (yellow), and ‘Transparency & \\nPublic Engagement’ (light blue) converge. This suggests that public communications fold diverse \\nconcerns into a single, accessible frame of ‘AI Safety’, with safety as the gravitational hub. \\nPublication clusters are more distinct: ‘Policy, Law & Regulation’ (purple) and ‘Governance & \\nResponsible Practices’ (green) form clearly bounded regions, while ‘Safety Engineering & Reliability’ \\n(grey), ‘Risk Management & High-Stakes Concerns’ (pink), and ‘Harm & Content Safety’ (red) \\noverlap partially. Here, safety does not dominate but operates more as a proxy that interacts with \\nrisk, harm, governance across different subfields.  \\nDiscursive Pivots: Safety and Policy \\nIn the corpus-wide PCA, ethics never operates as an organizing category. OpenAI’s discourse \\nadheres around two pivots: safety, which consolidates and institutionalizes over time, and policy, \\nwhich undergoes a semantic drift from technical reinforcement learning (RL) to governance \\nvocabulary. \\nThe trajectory of safety is clearest in the web corpus, where n-grams make visible a shift from \\nscattered rhetorical fragments to systematic institutionalization. In the early years (2016–2018), \\nmentions were rare (‘concrete safety problem,’ ‘people safety’), compared with RL discourse. By \\n2019, however, safety had formalized, with OpenAI’s Safety Gym platform benchmarks (‘safety gym \\nenvironment’) anchoring technical usage while phrases like ‘safety policy’ and ‘safety requirement’ \\ngestured toward governance. From 2020 to 2022, the vocabulary diversified into evaluation and \\ncompliance (‘safety bounty,’ ‘safety incident’), before spiking five-fold in 2023–2024, when safety \\nattached itself to risk, governance, and organizational frames (‘safety risk,’ ‘safety security \\ncommittee’). By mid-2025, the term remained saturated with specification language (‘safety check,’ \\n‘safety testing’), emphasizing its role as a procedural proxy. \\nThese granular shifts in OpenAI’s web articles triangulate observations that safety scales flexibly \\nacross different registers. On the site, it acts as the gravitational hub that absorbs adjacent \\nconcerns into a unified ‘AI safety’ narrative. In publications, it connects specialized subfields, like \\nrisk management, harm/content safety, reliability, without anchoring them. Taken together, safety \\nemerges not as a marker of ethics than but as a signal for compliance, governance, and evaluation. \\nPolicy offers different insight. Both web articles and publications register an anomalous surge in \\n2017–2018 (143 and 200+ mentions, respectively), when policy was dominated by reinforcement \\nlearning usage (‘policy gradient,’ ‘train policy,’ ‘policy head’). Mentions decline sharply in 2019–\\n2022, then resurge in 2023–2024 (92 and 122 mentions on the site; 200+ in publications). During \\nthis later period, semantics shift as governance and compliance uses take hold, through phrases \\nsuch as ‘safety policy,’ ‘usage policy,’ and ‘policy proposals’. Heatmaps for both web articles and \\npublications confirm that this is a corpus-wide pattern, policy’s trajectory reflects a broader \\nreorientation.  \\nDiscussion \\nOpenAI’s Public Discourse as a Signal about Ethics in Practice \\nAddressing our research question, as OpenAI absorbed ethics into a safety and alignment framing, \\nit unified development by encouraging buy-in from technological teams, investors, and partners. \\nHowever, it jeopardized societal and justice-oriented dimensions, diverging considerably from its \\ninitial mission. This transition aligned with increasing privatization and leveraging of IP to assert \\nInformation Research, Vol. ?? No. ? (20??) \\n11 \\npower and control over AI development. These changes, rooted in discourse, significantly \\nimpacted perceptions of what and who are critical and what or who is peripheral or expendable. \\nFor example, emphasis on ‘safety’ privileges research and staff that focus on technical aspects or \\nrisk and compliance, rather than a diverse array of values or ethical principles. Focus on ‘alignment’ \\ncenters ML and technosolutionism, alienating affected communities, civil society, social science \\nexperts. \\nImplications \\nThe case study of OpenAI highlights divergence between meaningful ethics dialogues and industry \\nassurances on safety, trustworthiness, and human impact of AI. OpenAI increasingly omits an \\nethics vocabulary, which can quietly reshape AI governance practices on compliance, risk, and \\nresponsibility. Without ethically-grounded governance, AI oversight is sparse, addressing only \\ntechnical safety or market-driven priorities. \\nOpenAI is a useful case study to understand how ethics-washing shapes norms and broader \\ndiscourse, in addition to how it obscures and distracts from practice. Internally, there is also a \\nsignificant impact of dialogue and framing on the design process, policy drafting, product safety \\nprocesses, and priorities in team membership, reflecting a lack of coherent institutional ethics. \\nRhetoric actively redistributes epistemic authority in the AI ecosystem and governance. This \\nhighlights the need for transparency and accountability measures that are not subject to \\norganizational dynamics or priorities; as evidenced by the reinstatement of Sam Altman as \\nOpenAI’s CEO on November 22, 2023 (Metz et al., 2023; Sam Altman returns as CEO, OpenAI has a \\nnew initial board, 2023), governance arrangements that are subject to pressure or capture are not \\ninstitutionally sound. \\nFuture Directions \\nGiven the significant shifts at OpenAI during this period of rapid AI innovation and adoption in \\nethics dialogue, parallel research ought to evaluate dialogue within the broader industry, \\nconsidering trends by firm, market establishment, and across geopolitical divides. Further, \\nresearchers should empirically consider how these statements correspond with technical \\npractices and compliance with evolving AI governance and regulation. \\nConclusions \\nThis research highlights the shifts over time toward emphasis on safety, risk, and compliance, \\nrather than nuanced ethos or meaningful social benefits. Results identified significant ethics \\nwashing, in line with recent research (e.g., Hao, 2025).  \\nIn evaluating OpenAI’s public discourse and the associated signals about ethics in practice, there \\nare broader implications around how language choices steer external accountability and impacts \\nregulatory exposure and assurance posture. As such, a core takeaway from this case study is that \\ngovernance and accountability for AI must be exogenously imposed and reflect nuanced ethical \\nframeworks. Multistakeholder dialogue regarding AI ethics must be robust and sustained. \\nEphemeral and episodic events or partnerships are insufficient to ensure that human values and \\nethical concerns are centred in the development and deployment of AI innovations. While \\nacademics may coalesce around particular frameworks, such as FATE or FAccT, their nuanced \\nconsiderations are not effectively transferred to industry practices or communications. Future \\nresearch must compare this case to the broader industry and its impacts on society. \\nAcknowledgements \\nThe authors are grateful for the support of the School of Information Sciences at the University of \\nIllinois at Urbana-Champaign and constructive feedback from reviewers and iConference 2026 \\nprogram committee members. \\nInformation Research, Vol. ?? No. ? (20??) \\n12 \\nAbout the author(s) \\nMelissa Wilfley is a Ph.D. student in Information Sciences at the University of Illinois Urbana-\\nChampaign (UIUC). She brings over 20 years professional experience in the technology sector and \\nholds an A.B. in Sociocultural Anthropology from the University of California, Davis. Her research \\nexamines AI ethics, governance and tech labour dynamics in global contexts. She can be contacted \\nat wilfley2@illinois.edu \\nMengting Ai is a Ph.D. student in Information Sciences at the University of Illinois Urbana-\\nChampaign (UIUC). They received their M.S. in Computer Science from UIUC. Their research \\ninterests include AI ethics and large language models. They can be contacted at mai10@illinois.edu \\nMadelyn Rose Sanfilippo is an Assistant Professor in the School of Information Sciences at the \\nUniversity of Illinois Urbana-Champaign. She received her Ph.D. from Indiana University and her \\nresearch addresses governance in sociotechnical systems. She can be contacted at \\nmadelyns@illinois.edu \\n \\n \\n \\nInformation Research, Vol. ?? No. ? (20??) \\n13 \\nReferences \\nBarocas, S., Hardt, M., & Narayanan, A. (2023). Fairness and machine learning: Limitations and \\nopportunities. The MIT Press. \\nBietti, E. (2020). From ethics washing to ethics bashing: A view on tech ethics from within moral \\nphilosophy. Proceedings of the 2020 Conference on Fairness, Accountability, and \\nTransparency, 210–219. https://doi.org/10.1145/3351095.3372860 \\nBurns, C., Leike, J., Aschenbrenner, L., Wu, J., Izmailov, P., Gao, L., Baker, B., & Kirchner, J. H. \\n(2023, December 14). Weak-to-strong generalization. \\nhttps://web.archive.org/web/20251122235243/https://openai.com/index/weak-to-\\nstrong-generalization/  \\nCath, C. (2018). Governing artificial intelligence: Ethical, legal and technical opportunities and \\nchallenges. Philosophical Transactions of the Royal Society A: Mathematical, Physical and \\nEngineering Sciences, 376(2133), 20180080. https://doi.org/10.1098/rsta.2018.0080 \\nFairclough, N. (1992). Discourse and text: Linguistic and intertextual analysis within discourse \\nanalysis. Discourse & Society, 3(2), 193–217. https://doi.org/10.1177/0957926592003002004 \\nFloridi, L., & Cowls, J. (2019). A unified framework of five principles for AI in society. Harvard Data \\nScience Review. https://doi.org/10.1162/99608f92.8cd550d1 \\nGebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., & Crawford, K. \\n(2021). Datasheets for datasets. Communications of the ACM, 64(12), 86–92. \\nhttps://doi.org/10.1145/3458723 \\nGreen, B. (2021). The contestation of tech ethics: A sociotechnical approach to technology ethics \\nin practice. Journal of Social Computing, 2(3), 209–225. \\nhttps://doi.org/10.23919/JSC.2021.0018 \\nGreene, D., Hoffmann, A. L., & Stark, L. (2019). Better, nicer, clearer, fairer: A critical assessment of \\nthe movement for ethical artificial intelligence and machine learning. Hawaii International \\nConference on System Sciences. https://doi.org/10.24251/HICSS.2019.258 \\nGriffin, T. A., Green, B. P., & Welie, J. V. M. (2025). The ethical wisdom of AI developers. AI and \\nEthics, 5(2), 1087–1097. https://doi.org/10.1007/s43681-024-00458-x \\nGuan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., \\nWei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., & Glaese, A. (2025). Deliberative \\nalignment: Reasoning enables safer language models (No. arXiv:2412.16339). arXiv. \\nhttps://doi.org/10.48550/arXiv.2412.16339 \\nHajer, M. A. (1995). The politics of environmental discourse: Ecological modernization and the policy \\nprocess. Clarendon Press ; Oxford University Press. \\nHao, K. (2025). Empire of AI: Dreams and nightmares in Sam Altman’s OpenAI. Penguin Press.  \\nHeilinger, J.-C. (2022). The ethics of ai ethics. A constructive critique. Philosophy & Technology, \\n35(3), 61. https://doi.org/10.1007/s13347-022-00557-9 \\nHerzog, C., & Blank, S. (2024). A systemic perspective on bridging the principles-to-practice gap \\nin creating ethical artificial intelligence solutions – a critique of dominant narratives and \\nproposal for a collaborative way forward. Journal of Responsible Innovation, 11(1), 2431350. \\nhttps://doi.org/10.1080/23299460.2024.2431350 \\nInformation Research, Vol. ?? No. ? (20??) \\n14 \\nIEEE. (2019). Ethically Aligned Design - A Vision for Prioritizing Human Well-Being with \\nAutonomous and Intelligent Systems, 1–294. \\nhttps://ieeexplore.ieee.org/document/9398613 \\nIrving, G., & Askell, A. (2019). AI safety needs social scientists. Distill, 4(2), 10.23915/distill.00014. \\nhttps://doi.org/10.23915/distill.00014 \\nJobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature \\nMachine Intelligence, 1(9), 389–399. https://doi.org/10.1038/s42256-019-0088-2 \\nLazar, S., & Nelson, A. (2023). AI safety on whose terms? Science, 381(6654), 138–138. \\nhttps://doi.org/10.1126/science.adi8982 \\nLloyd, S. (1982). Least squares quantization in PCM. IEEE transactions on information theory, \\n28(2), 129-137.  \\nMcInnes, L., Healy, J., & Astels, S. (2017). hdbscan: Hierarchical density based clustering. J. Open \\nSource Software, 2(11), 205. https://joss.theoj.org/papers/10.21105/joss.00205 \\nMetcalf, J., Moss, E., & boyd, danah. (2019). Owning ethics: Corporate logics, Silicon Valley, and \\nthe institutionalization of ethics. Social Research: An International Quarterly, 86(2), 449–\\n476. https://doi.org/10.1353/sor.2019.0022 \\nMetcalf, J., Moss, E., Watkins, E. A., Singh, R., & Elish, M. C. (2021). Algorithmic impact assessments \\nand accountability: The co-construction of impacts. Proceedings of the 2021 ACM \\nConference on Fairness, Accountability, and Transparency, 735–746. \\nhttps://doi.org/10.1145/3442188.3445935 \\nMetz, C., Isaac, M., Mickle, T., Weise, K., & Roose, K. (2023, November 22). Sam Altman Is \\nReinstated as OpenAI’s Chief Executive. The New York Times. Retrieved January 2, 2026. \\nhttps://web.archive.org/web/20250912211038/https://www.nytimes.com/2023/11/22/\\ntechnology/openai-sam-altman-returns.html \\nMitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D., & \\nGebru, T. (2019). Model cards for model reporting. Proceedings of the Conference on \\nFairness, Accountability, and Transparency, 220–229. \\nhttps://doi.org/10.1145/3287560.3287596 \\nMittelstadt, B. (2019). Principles alone cannot guarantee ethical AI. Nature Machine Intelligence, \\n1(11), 501–507. https://doi.org/10.1038/s42256-019-0114-4 \\nMorley, J., Elhalal, A., Garcia, F., Kinsey, L., Mökander, J., & Floridi, L. (2021). Ethics as a Service: A \\npragmatic operationalisation of AI ethics. Minds and Machines, 31(2), 239–256. \\nhttps://doi.org/10.1007/s11023-021-09563-w \\nOpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., \\nAltenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., \\nBaltescu, P., Bao, H., Bavarian, M., Belgum, J., … Zoph, B. (2024). Gpt-4 technical report \\n(arXiv:2303.08774). arXiv. https://doi.org/10.48550/arXiv.2303.08774 \\nOpenAI appoints Scott Schools as Chief Compliance Officer. (2024, October 22). OpenAI. Retrieved \\nSeptember 12, 2025. \\nhttps://web.archive.org/web/20250810102318/https://openai.com/global-\\naffairs/openai-chief-compliance-officer-announcement/ \\nInformation Research, Vol. ?? No. ? (20??) \\n15 \\nOpenAI’s approach to AI and national security. (2024, October 24). OpenAI. Retrieved November \\n20, 2025. https://web.archive.org/web/20251112113838/https://openai.com/global-\\naffairs/openais-approach-to-ai-and-national-security/ \\nOpenAI charter. (n.d.). OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250912002726/https://openai.com/charter/ \\nOpenAI news. (2025, August 7). OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250808015203/https://openai.com/news/ \\nOpenAI research. (2025, September 5). OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250723120604/https://openai.com/research/index/ \\nOpenAI’s commitment to child safety: Adopting safety by design principles. (2024, April 23). OpenAI. \\nRetrieved September 12, 2025. \\nhttps://web.archive.org/web/20250910175627/https://openai.com/index/child-safety-\\nadopting-sbd-principles/ \\nOperator system card. (2025, January 23). OpenAI. Retrieved December 20, 2025. \\nhttps://web.archive.org/web/20250910181725/https://openai.com/index/operator-\\nsystem-card/ \\nOur updated preparedness framework. (2025, April 15). OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250928131055/https://openai.com/index/updating-\\nour-preparedness-framework/ \\nOrr, W., & Davis, J. L. (2020). Attributions of ethical responsibility by Artificial Intelligence \\npractitioners. Information, Communication & Society, 23(5), 719–735. \\nhttps://doi.org/10.1080/1369118X.2020.1713842 \\nPapagiannidis, E., Mikalef, P., & Conboy, K. (2025). Responsible artificial intelligence governance: \\nA review and research framework. The Journal of Strategic Information Systems, 34(2), \\n101885. https://doi.org/10.1016/j.jsis.2024.101885 \\nPioneering an AI clinical copilot with Penda Health. (2025, July 22). OpenAI. Retrieved September \\n12, 2025. https://web.archive.org/web/20250916204527/https://openai.com/index/ai-\\nclinical-copilot-penda-health/ \\nPlanning for AGI and beyond. (2023, February 24). OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250928123016/https://openai.com/index/planning-\\nfor-agi-and-beyond/ \\nRaji, I. D., Smart, A., White, R. N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, \\nD., & Barnes, P. (2020). Closing the AI accountability gap: Defining an end-to-end \\nframework for internal algorithmic auditing. Proceedings of the 2020 Conference on \\nFairness, Accountability, and Transparency, 33–44. \\nhttps://doi.org/10.1145/3351095.3372873 \\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. ArXiv. https://arxiv.org/abs/1908.10084 \\nRyan, M., Antoniou, J., Brooks, L., Jiya, T., Macnish, K., & Stahl, B. (2021). Research and practice of \\nAI ethics: A case study approach juxtaposing academic discourse with organisational \\nreality. Science and Engineering Ethics, 27(2), 16. https://doi.org/10.1007/s11948-021-\\n00293-x \\nInformation Research, Vol. ?? No. ? (20??) \\n16 \\nSam Altman returns as CEO, OpenAI has a new initial board. (2023, November 29). OpenAI. \\nRetrieved December 30, 2025. \\nhttps://web.archive.org/web/20251213233127/https://openai.com/index/sam-altman-\\nreturns-as-ceo-openai-has-a-new-initial-board/  \\nSeele, P., & Schultz, M. D. (2022). From greenwashing to machinewashing: A model and future \\ndirections derived from reasoning by analogy. Journal of Business Ethics, 178(4), 1063–1089. \\nhttps://doi.org/10.1007/s10551-022-05054-9 \\nShilton, K. (2018). Values and ethics in human-computer interaction. Foundations and Trends in \\nHuman–Computer Interaction, 12(2), 107–171. https://doi.org/10.1561/1100000073 \\nShoker, S., & Reddie, A. (2023, August 1). Confidence-building measures for artificial intelligence: \\nWorkshop proceedings. OpenAI. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20251113040959/https://openai.com/index/confidence-\\nbuilding-measures-for-artificial-intelligence/ \\nSora system card. (2024, December 9). OpenAI. Retrieved December 23, 2025. \\nhttps://web.archive.org/web/20250912002625/https://openai.com/index/sora-\\nsystem-card/ \\nStahl, B. C., Antoniou, J., Ryan, M., Macnish, K., & Jiya, T. (2022). Organisational responses to the \\nethical issues of artificial intelligence. AI & SOCIETY, 37(1), 23–37. \\nhttps://doi.org/10.1007/s00146-021-01148-6 \\nStamboliev, E., & Christiaens, T. (2025). How empty is trustworthy AI? A discourse analysis of the \\nethics guidelines of trustworthy AI. Critical Policy Studies, 19(1), 39–56. \\nhttps://doi.org/10.1080/19460171.2024.2315431 \\nU.S. Senate Committee on the Judiciary, Subcommittee on Privacy, Technology, and the Law. \\n(2023, May 16). Oversight of A.I.: Rules for artificial intelligence [Hearing]. U.S. Government \\nPublishing Office. Retrieved September 12, 2025. \\nhttps://web.archive.org/web/20250913112015/https://www.judiciary.senate.gov/commi\\nttee-activity/hearings/oversight-of-ai-rules-for-artificial-intelligence \\nvan Maanen, G. (2022). AI ethics, ethics washing, and the need to politicize data ethics. Digital \\nSociety, 1(2), 9. https://doi.org/10.1007/s44206-022-00013-3 \\n \\n \\n \\n'), Document(metadata={'Published': '2023-07-07', 'Title': 'AI and the EU Digital Markets Act: Addressing the Risks of Bigness in Generative AI', 'Authors': 'Ayse Gizem Yasar, Andrew Chong, Evan Dong, Thomas Krendl Gilbert, Sarah Hladikova, Roland Maio, Carlos Mougan, Xudong Shen, Shubham Singh, Ana-Andreea Stoica, Savannah Thais, Miri Zilka', 'Summary': \"As AI technology advances rapidly, concerns over the risks of bigness in digital markets are also growing. The EU's Digital Markets Act (DMA) aims to address these risks. Still, the current framework may not adequately cover generative AI systems that could become gateways for AI-based services. This paper argues for integrating certain AI software as core platform services and classifying certain developers as gatekeepers under the DMA. We also propose an assessment of gatekeeper obligations to ensure they cover generative AI services. As the EU considers generative AI-specific rules and possible DMA amendments, this paper provides insights towards diversity and openness in generative AI services.\"}, page_content='arXiv:2308.02033v1  [cs.CY]  7 Jul 2023\\nAI and the EU Digital Markets Act:\\nAddressing the Risks of Bigness in Generative AI\\nAyse Gizem Yasar 1 Andrew Chong * 2 Evan Dong * 3 Thomas Krendl Gilbert * 4 Sarah Hladikova * 5\\nRoland Maio * 6 Carlos Mougan * 7 Xudong Shen * 8 Shubham Singh * 9 Ana-Andreea Stoica * 10\\nSavannah Thais * 6 Miri Zilka * 11\\nAbstract\\nAs AI technology advances rapidly, concerns\\nover the risks of bigness in digital markets\\nare also growing.\\nThe EU’s Digital Markets\\nAct (DMA) aims to address these risks.\\nStill,\\nthe current framework may not adequately\\ncover generative AI systems that could become\\ngateways for AI-based services.\\nThis paper\\nargues for integrating certain AI software as\\n“core platform services” and classifying certain\\ndevelopers as gatekeepers under the DMA. We\\nalso propose an assessment of gatekeeper obliga-\\ntions to ensure they cover generative AI services.\\nAs the EU considers generative AI-speciﬁc rules\\nand possible DMA amendments, this paper\\nprovides insights towards diversity and openness\\nin generative AI services.\\n1. Introduction\\nThe European Union’s (EU) response to “bigness” (Bran-\\ndeis, 1934; Wu, 2018) in digital markets goes beyond tra-\\nditional antitrust and competition law enforcement. Under\\nEU competition law, large companies have not been viewed\\nas inherently problematic. However, there is an increasing\\nconcern that “a few large platforms increasingly act as gate-\\nways or gatekeepers between business users and end users\\nand enjoy an entrenched and durable position, often as a\\n*Authors ordered alphabetically, except the ﬁrst author. 1 Lon-\\ndon School of Economics, London, United Kingdom 2University\\nof California, Berkeley, USA 3Cornell University,USA 4Cornell\\nTech, USA 5Tufts University, USA 6Columbia University, USA\\n7University of Southampton, United Kingdom 8National Univer-\\nsity of Singapore, Singapore 9University of Illinois Chicago, USA\\n10Max Planck Institute for Intelligent Systems, T¨ubingen, Ger-\\nmany 11University of Cambridge, UK. Correspondence to: Ayse\\nGizem Yasar <a.g.yasar@lse.ac.uk>.\\nProceedings of the 40 th International Conference on Machine\\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\\n2023 by the author(s).\\nresult of the creation of conglomerate ecosystems around\\ntheir core platform services, which reinforces existing en-\\ntry barriers.” (European Commission, 2020). According to\\nthe European Commission, these entrenched positions lead\\nto unfair behaviour vis-`a-vis business users of these plat-\\nforms, as well as reduced innovation and contestability in\\ncore platform services.\\nThese concerns culminated in the creation of a new Euro-\\npean regulation that goes beyond traditional competition\\nlaw rules: the Digital Markets Act (DMA). The DMA is\\nset up to counteract platform size and gatekeeping rather\\nthan abuse of dominance or monopoly power that competi-\\ntion/antitrust laws target. It aims to address the shortcom-\\nings of competition law in keeping the entry barriers low\\nand ensuring fair game between “gatekeepers” and their\\nsmaller rivals that depend on gatekeepers’ services. The\\nregulation covers some well-established platform services\\nlike operating systems, messaging platforms, and online ad-\\nvertising.\\nThe DMA has been applauded by many who are concerned\\nabout the rise of large digital platforms. It has also been crit-\\nicised because it may no longer be possible to inject diver-\\nsity into digital markets once certain players have become\\nsquarely entrenched. This is a concern that we share. The\\nDMA is silent on AI, but we observe that gatekeepers are\\nstarting to emerge in generative AI applications. The DMA\\nprovides an opportunity to maintain a fair and diverse space\\nfor AI applications before a few large players become en-\\ntrenched and durable. There is now political momentum in\\nboth the EU and the US to address the emergence of gen-\\nerative AI gatekeepers (Vestager, 2023; Subcommittee on\\nPrivacy, Technology, and the Law, 2023)\\nThis paper argues that the generative AI industry should be\\ndirectly addressed in the DMA. In particular, we argue that\\ngenerative AI services should be integrated into the DMA’s\\nlist of core platform services. We show that certain gener-\\native AI services embody gatekeeper characteristics in the\\nsense of the DMA. While certain use cases of generative\\nAI might indirectly fall under the DMA’s current list of\\ncore platform services, there are complex ways in which\\nAddressing the Risks of Bigness in Generative AI\\ngenerative AI services may act as gatekeepers in their own\\nright. Among those, we highlight the gatekeeping potential\\nof generative AI service providers due to: (i) computing\\npower, (ii) early mover advantages, and (iii) data resources\\nand integrated systems. We conclude with a discussion on\\nhow the DMA could be amended to ensure contestability\\nand fairness in the market for generative AI services.\\n2. Gatekeepers and the DMA\\nThe DMA came into force around the same time as the Dig-\\nital Services Act, which regulates platform accountability\\nand content moderation. Both regulations impose speciﬁc\\nobligations on companies above certain size thresholds, al-\\nbeit framed and described differently. The EU is also about\\nto ﬁnalise its AI Act, which adopts a risk-based approach.\\nIt outright prohibits certain AI applications that bear un-\\nacceptable risks to people’s safety, and introduces trans-\\nparency and accountability rules for high-risk applications.\\nThe DMA targets “gatekeeper” companies, which is\\ndeﬁned under Article 3(1) as an undertaking that: (i) “has\\na signiﬁcant impact on the [EU’s] internal market”,\\n(ii) “provides a core platform service which is an important\\ngateway for business users to reach end-users”, and\\n(iii) “enjoys an entrenched and durable position in its\\noperations, or it is foreseeable that it will enjoy such a\\nposition in the near future.”\\nThe current list of “core platform services” provided in Ar-\\nticle 2(2) covers ten established digital services, such as op-\\nerating systems, web browsers, and social networking. It\\ndoes not explicitly cover generative AI services. In the rest\\nof this paper, we bring forward ways in which generative\\nAI can be provided as a platform service and argue for its\\nexplicit integration into the DMA.\\n3. Platformization of AI\\nThe DMA applies to core platform services.\\nAlthough\\nsome companies are likely to offer AI only as a product, an-\\nother potentially effective route to proﬁt is to provide gen-\\nerative AI as a platform. For example, OpenAI has created\\nseveral large foundation models (e.g., GPT-4 and DALL-E)\\nthat can serve as the basis for a wide range of applications.\\nThe company began to monetize these foundational models\\nin different ways, including: (i) by releasing some models\\ndirectly to the public (e.g., ChatGPT) using a “freemium”\\nbusiness model, and (ii) by offering API access to its mod-\\nels and enabling the development of applications built on\\ntop of them. The latter allows organizations to integrate\\nOpenAI’s models into their own products, which they then\\nprovide to the public. To the extent generative AI applica-\\ntions are provided as a platform, they can be brought within\\nthe remit of the DMA.\\n4. Emergence of Gatekeepers in Generative\\nAI\\nThe generative AI industry is already driven by a small\\nnumber of companies, notably OpenAI, Google, Microsoft,\\nand Meta, who hold a signiﬁcant competitive advantage\\ndue to their extensive data resources, specialized hardware\\narchitectures, vertical integration, network effects, ﬁnan-\\ncial clout, know-how, and early-mover advantage. While\\nsome of these advantages are speciﬁc to AI applications,\\nsuch as specialized hardware architectures, most—such as\\ndata resources, ﬁnancial clout, network effects, integration\\nand the importance of early movers—have been present in\\ndigital markets in general and given rise to the gatekeeping\\npositions that the DMA now seeks to address (European\\nCommission, 2020). Without regulatory intervention, these\\nsigniﬁcant advantages will likely turn into entrenched posi-\\ntions in generative AI services, as they have in other digital\\nmarkets.\\nComputing Power in the Hands of a Few: While the\\ncost of ﬁne-tuning large generative AI models is decreasing,\\nachieving state-of-the-art performance still requires a high\\nbudget, thus creating an entry barrier for potential players\\nand inhibiting diversity and market growth. This entry bar-\\nrier disproportionately affects smaller companies, public in-\\nstitutions, and universities, who often lack the ﬁnancial re-\\nsources to establish independent large-scale generative AI\\nsystems. Consequently, the concentration of cutting-edge\\ncomputing power and expertise in the hands of a few play-\\ners risks limiting player diversity and stiﬂing innovation\\nwithin the generative AI industry.\\nEarly Mover Advantage: Early movers’ head start, like\\nOpenAI and DeepMind, in developing generative AI sys-\\ntems may also lead to entrenched positions. Early movers\\ncurrently face the challenge of determining whether they\\nshould make their AI systems available as open source (Vin-\\ncent, 2023). However, even if they do, such open-source\\nversions often come with strings attached to enable mone-\\ntization, which was the core dispute in the European Com-\\nmission’s Google Android case in the context of mobile\\noperating systems (European Commission, 2022). In any\\ncase, monetization is now becoming increasingly common\\nfor developers (Dastin et al., 2023).\\nData Resources and Integrated Systems:\\nA new trend\\nin generative AI systems is the recent inﬂux of integrated\\nservices:\\nsearch engines integrate LLMs, personal as-\\nsistants, note-taking, editing, creative tasks automation,\\nvideo-editing applications, or generative AI-augmented\\nsearch (Reid, 2023). As integrated systems that use gen-\\nerative AI become ubiquitous, their convenience and poten-\\ntial for creative endeavours trade off with user autonomy.\\nMeanwhile, platform tendency for integration and the asso-\\nciated loss of user autonomy is not new: Google, Microsoft\\nAddressing the Risks of Bigness in Generative AI\\nand Apple have all pushed for integrated systems in their\\nnow-established services. Furthermore, large players may\\nbeneﬁt from both existing troves of data from legacy appli-\\ncations (e.g., Google from G-suite users), as well as data\\ngenerated as users interact with AI applications (through\\nprompts and other inputs). The more popular an applica-\\ntion is, the more it will beneﬁt from human feedback, which\\ncreates feedback loops associated with network effects—\\nanother factor motivating the DMA. As a result of service\\nand data integration, it becomes increasingly difﬁcult for a\\nuser to switch between platforms and transport their data\\nand projects. The DMA speciﬁcally prohibits data and ser-\\nvice integration across different offerings of core platform\\nservices (e.g., Article 5(2)(b) and 5(8)). However, as men-\\ntioned above, this list does not include any generative AI\\napplications.\\n5. Contestability and Fairness in the\\nGenerative AI Industry\\nThe potential harms of AI have been discussed for years,\\nbut the current race toward generative AI enhancement\\nmight lead to the emergence of a few large players at the\\nexpense of contestability and fairness in the generative AI\\nindustry. The draft AI Act and the DMA do not directly\\naddress this problem.\\nCertain cases of generative AI applications might indirectly\\nfall within the remit of the DMA when gatekeepers inte-\\ngrate proprietary or third-party generative AI into their core\\nplatform services. However, such indirect application falls\\nshort of addressing our concerns. The DMA’s gatekeeper\\nobligations that are designed to counteract contestability\\nand fairness issues will not apply to their generative AI of-\\nfering when it is provided as a standalone platform service.\\nThere is evidence that unfair practices are already emerg-\\ning in generative AI. For example, Microsoft has reportedly\\nthreatened to cut off the access of at least two of its search\\nindex business customers that were building their own gen-\\nerative AI tools using data from the search index (Nylen\\n& Bass, 2023). Such behavior appears to be intended to\\nweaken the contestability of Microsoft’s own generative AI\\noffering by denying potential competitors the key input fac-\\ntor of data. Such practices are exactly what the DMA aims\\nto eliminate, but they are currently not caught by the reg-\\nulation. Generative AI providers remain free to leverage\\ntheir superior bargaining power to engage in such unfair\\npractices, and their already considerable economic power\\nto prevent contestability.\\nThe DMA provides an opportunity to address contestability\\nand fairness issues in the generative AI industry by desig-\\nnating certain AI software as core platform services and\\ncertain developers as gatekeepers. An initial assessment of\\ngatekeeper obligations under the DMA (Articles 5-6-7) re-\\nveals that some of them would already apply to generative\\nAI systems. For example, under Article 6(2), gatekeepers\\nare prevented from using, in competition with their busi-\\nness users, the data generated by business users of their\\ncore platform services and by these businesses’ customers.\\nThis provision would prevent generative AI gatekeepers\\nfrom free-riding on data produced by businesses relying\\non their API to provide downstream applications, either\\nin generative AI verticals or other industries. Similarly, a\\ngenerative AI gatekeeper would be prevented from forcing\\nbusiness users to rely on its own identiﬁcation services,\\nweb browser engine, payment service, or other technical\\nservices, for services provided using the gatekeeper’s gen-\\nerative AI under Article 5(7). A similar provision under\\nArticle 5(8) would prevent generative AI gatekeepers from\\nforcing businesses and end users to subscribe to or register\\nwith any of its other core platform services as a condition\\nto use its generative AI service.\\nThe DMA can thus support contestability and innovation in\\nAI systems, promoting the development of a more diverse\\nand accessible generative AI market.\\nIn conclusion, this paper presents a set of proposals to\\namend the DMA to prevent the emergence of entrenched\\npositions and to address potential harms related to gate-\\nkeeping in the generative AI industry.\\nAddressing the Risks of Bigness in Generative AI\\nReferences\\nBrandeis, L. D. The Curse of Bigness: Miscellaneous Papers of Louis D. Brandeis. Viking Press, 1934.\\nDastin, J., Hu, K., and Dave, P. Exclusive: Chatgpt owner openai projects $1 billion in revenue by 2024. Reuters, 2023.\\nURL https://www.reuters.com/business/chatgpt-owner-openai-projects-1-billion-revenue-by-202\\nEuropean Commission.\\nProposal for a regulation of the european parliament and of the council on con-\\ntestable and fair markets in the digital sector (digital markets act).\\nCOM/2020/842 ﬁnal, 2020.\\nURL\\nhttps://eur-lex.europa.eu/legal-content/en/TXT/?uri=COM%3A2020%3A842%3AFIN.\\nEuropean\\nCommission.\\nGoogle\\nand\\nAlphabet\\nv\\nCommission\\n(Google\\nAndroid),\\n2022.\\nURL\\nhttps://ec.europa.eu/competition/antitrust/cases/dec_docs/40099/40099_9993_3.pdf.\\nNylen, L. and Bass, D.\\nMicrosoft threatens data restrictions in rival ai search.\\nBloomberg, 2023.\\nURL\\nhttps://finance.yahoo.com/news/microsoft-threatens-restrict-data-rival-002746878.html.\\nReid,\\nE.\\nSupercharging\\nsearch\\nwith\\ngenerative\\nai.\\nGoogle,\\n2023.\\nURL\\nhttps://blog.google/products/search/generative-ai-search/.\\nSubcommittee on Privacy, Technology, and the Law. Oversight of a.i.: Rules for artiﬁcial intelligence. Subcommittee Hear-\\ning, 2023. URL https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-rule\\nDirksen Senate Ofﬁce Building Room 226, Washington, D.C.\\nVestager,\\nM.\\nEu eyes new rules for generative ai this year.\\nNikkei Asie Interview,\\n2023.\\nURL\\nhttps://asia.nikkei.com/Editor-s-Picks/Interview/EU-eyes-new-rules-for-generative-AI-this-y\\nVincent, J. Openai co-founder on company’s past approach to openly sharing research: “we were wrong”, Mar 2023. URL\\nhttps://www.theverge.com/2023/3/15/23640180/openai-gpt-4-launch-closed-research-ilya-sutske\\nWu, T. The curse of bigness. Columbia Global Reports, 75, 2018.\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs=ArxivLoader(query=\"Generative AI\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "171d8155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2016-05-26',\n",
       " 'Title': 'Heat-bath random walks with Markov bases',\n",
       " 'Authors': 'Caprice Stanley, Tobias Windisch',\n",
       " 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of allowed moves of arbitrary length. We show that the diameter of these graphs on fibers of a fixed integer matrix can be bounded from above by a constant. We then study the mixing behaviour of heat-bath random walks on these graphs. We also state explicit conditions on the set of moves so that the heat-bath random walk, a generalization of the Glauber dynamics, is an expander in fixed dimension.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs\n",
    "docs[0].metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
